\documentclass[conf]{new-aiaa}
%\documentclass[journal]{new-aiaa} for journal papers
\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[version=4]{mhchem}
\usepackage{siunitx}
\usepackage{amssymb}
\usepackage{longtable,tabularx}
\usepackage{booktabs}

\usepackage{acro}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{subcaption}


\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{ifthen}
\usepackage{multirow}

\usetikzlibrary{matrix,calc}
% \addbibresource{bibs/ieee.bib}

\setlength\LTleft{0pt} 

\title{SeeNN: Leveraging Multimodal Deep Learning for In-Flight Long-Range Atmospheric Visibility Estimation in Aviation Safety}

\author{Taha Bouhsine \footnote{Graduate Research Fellow}, Giuseppina Carannante.\footnote{Postdoctoral Fellow}, Nidhal C. Bouaynaya.\footnote{Associate Dean for Research \& Graduate Studies and Professor of Electrical \& Computer Engineering}}
\affil{Electrical and Computer Engineering Department, Henery M.Rowan College of Engineering, Rowan University, Glassboro, New Jersey, 08028}
\author{Soufiane Idbraim \footnote{Computer Science Professor and Head of IRF-SIC Laboratory}}
\affil{IRF-SIC Laboratory, Computer Science Department, Faculty of Sciences Agadir, Ibn Zohr University, Agadir, Morocco}
\author{Phuong Tran, Grant Morfit, Maggie Mayfield, Charles Cliff Johnson}
\affil{William J. Hughes Technical Center, Federal Aviation Administration, Atlantic City, NJ, USA}

\begin{document}

\maketitle

\begin{abstract}
Accurate, real-time estimation of atmospheric visibility is a critical yet challenging task in aviation safety. While deep learning has shown promise, unimodal approaches relying solely on RGB imagery often fail to capture the complexity of atmospheric conditions, leading to limitations in reliability and accuracy. This paper introduces SeeNN, a novel multimodal deep learning framework designed for robust, long-range, in-flight visibility estimation. SeeNN integrates information from five diverse modalities: RGB imagery, depth maps, normal surface maps, edge maps, and entropy maps. To facilitate the development and evaluation of such models, we also present SeeSet V1, a new, comprehensive, and publicly available benchmark dataset featuring a wide range of altitudes, land covers, and visibility conditions. Our extensive experiments demonstrate the superiority of the multimodal approach. The SeeNN framework achieves a classification accuracy of over 97\%, a significant improvement upon the 87.92\% accuracy of a baseline unimodal RGB model. This work underscores the substantial potential of multimodal fusion to enhance the reliability of automated visibility estimation systems, representing a key advancement toward improving safety and operational efficiency in aviation and other domains where visibility is a critical factor.
\end{abstract}

% \section{Nomenclature}

% {\renewcommand\arraystretch{1.0}
% \noindent\begin{longtable*}{@{}l @{\quad=\quad} l@{}}
% $A$  & amplitude of oscillation \\
% $a$ &    cylinder diameter \\
% $C_p$& pressure coefficient \\
% $Cx$ & force coefficient in the \textit{x} direction \\
% $Cy$ & force coefficient in the \textit{y} direction \\
% c   & chord \\
% d$t$ & time step \\
% $Fx$ & $X$ component of the resultant pressure force acting on the vehicle \\
% $Fy$ & $Y$ component of the resultant pressure force acting on the vehicle \\
% $f, g$   & generic functions \\
% $h$  & height \\
% $i$  & time index during navigation \\
% $j$  & waypoint index \\
% $K$  & trailing-edge (TE) nondimensional angular deflection rate
% \end{longtable*}}

\input {chapters/intro}

\input{chapters/related_work}
\input{chapters/materials}
\input{chapters/results}
\input{chapters/conclusion}
\input{chapters/ack}
\bibliography{sample}




\end{document}
