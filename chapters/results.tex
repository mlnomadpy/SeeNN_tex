\section{Results and Discussion}
\label{sec:results}

This section presents the experimental results of our study. We begin by describing the performance of a unimodal RGB-based model, followed by a detailed analysis of the multimodal SeeNN framework. The discussion encompasses a comparative analysis of different fusion strategies, an examination of model performance through confusion matrices, and a consideration of computational costs and dataset limitations.

\subsection{Unimodal Model Performance}

A baseline model was established using only the RGB modality. This unimodal model achieved an overall accuracy of 87.92\% on the validation set. This performance, while reasonable, highlights the inherent limitations of relying on a single data source, particularly when tested on previously unseen views. To ensure the robustness of our evaluation and prevent data leakage, we employed a strict holdout validation strategy, as described in the Experimental Setup, where entire geographical locations were withheld from the training set. This rigorous approach prevents the model from overfitting to specific sceneries and provides a more realistic assessment of its generalization capabilities.

\subsection{Multimodal Model Performance}

In contrast to the unimodal baseline, the multimodal models developed within the SeeNN framework demonstrated a significant improvement in performance. As shown in Table~\ref{tab:res_table} and Figure~\ref{fig:conf_mats}, the fusion of multiple modalities resulted in a substantial increase in prediction accuracy, with gains of up to 10 percentage points.

For instance, a model combining RGB and Depth modalities, using a simple concatenation connector, achieved an accuracy of 97.57\%. The highest performing model, which fused all five modalities (RGB, Entropy, Edge, Depth, and Normal Surface) using a self-attention connector, reached an impressive validation accuracy of 97.63\%. These results strongly support the hypothesis that integrating diverse data sources enables the model to form a more comprehensive and robust understanding of the atmospheric conditions, leading to more accurate visibility estimations.

\begin{table*}
\centering
\caption{Ablation study comparing the performance of different modality combinations and fusion connectors. The highest accuracy for each connector type is highlighted in bold.}
\label{tab:res_table}
\begin{tabular}{@{}lccccccr@{}}
\toprule
Connector & RGB & Entropy & Edge & Depth &  Normal Surface  & \# Trainable Params. & Val. Acc. (\%)  \\
\midrule
Unimodal & \checkmark &  &  &  &   & 7M &  87.92  \\
\midrule
\multirow{5}{*}{Concatenate}& \checkmark & \checkmark &  &  &   & 14M &  96.40  \\
& \checkmark &   & \checkmark &  &   & 14M & 96.53   \\
& \checkmark &  &  & \checkmark &   & 14M &   \textbf{97.57}  \\
& \checkmark &  &  & \checkmark & \checkmark  & 21M &   97.14  \\
& \checkmark & \checkmark & \checkmark  & \checkmark & \checkmark  & 38M & 96.30  \\
\midrule
\multirow{4}{*}{Self-Attention} & \checkmark &   & \checkmark &  &   & 14M & 96.86  \\
& \checkmark &  &  & \checkmark &   & 14M &   96.31  \\
& \checkmark &  &  & \checkmark & \checkmark  & 21M &   97.47  \\
& \checkmark & \checkmark & \checkmark  & \checkmark & \checkmark  & 38M & \textbf{97.63}  \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Analysis of Misclassifications}

The confusion matrices presented in Figure~\ref{fig:conf_mats} provide further insight into the performance of the top-performing multimodal models. While the overall accuracy is high, the models exhibit some difficulty in distinguishing between adjacent visibility categories. Specifically, there is a tendency to misclassify instances of "Class 3" (3 to 5 miles visibility) as "Class 4" (>= 5 miles visibility). This suggests that the visual cues differentiating these two classes are subtle and challenging for the models to discern.

Interestingly, the combination of RGB and Depth modalities yielded improved performance for this specific class, indicating that depth information is particularly valuable for resolving ambiguity in this visibility range. Future work could explore the integration of additional modalities or the development of more sophisticated fusion mechanisms to address this specific challenge.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
    \include{chapters/confmats/all_attention}
    \caption{All Modalities (Self-Attention)}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
        \include{chapters/confmats/all_concat}
        \caption{All Modalities (Concatenate)}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.4\textwidth}
        \include{chapters/confmats/rgb_depth_concatenate}
        \caption{RGB + Depth (Concatenate)}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
        \include{chapters/confmats/rgb_depth_self_attention}
        \caption{RGB + Depth (Self-Attention)}
    \end{subfigure}
    \caption{Confusion matrices for the top-performing multimodal models. The models show high accuracy but struggle with adjacent classes, particularly misclassifying Class 3 as Class 4.}
    \label{fig:conf_mats}
\end{figure}

\subsection{Discussion}

\subsubsection{Computational Cost and Deployment Considerations}

A critical consideration in the practical application of these models is the trade-off between performance and computational cost. While the model that fused all available modalities achieved the highest accuracy, it also has the highest computational overhead, requiring the execution of multiple modality estimators and backbone networks. When deploying such models, particularly on resource-constrained hardware such as embedded devices, it is essential to consider these limitations. The results suggest that a carefully selected subset of modalities (e.g., RGB and Depth) can provide a favorable balance between accuracy and efficiency.

\subsubsection{Dataset Limitations and Future Directions}

While the SeeSet V1 dataset addresses a significant gap in the availability of public, multi-view datasets for atmospheric visibility research, it has certain limitations. The diversity of landscapes and land covers could be expanded to enhance the model's generalizability. Future work should focus on enriching the dataset using the latest generation of flight simulators (e.g., Microsoft Flight Simulator, X-Plane 12), which offer near-photorealistic rendering and more sophisticated atmospheric models.

Further research should also explore advanced pre-training techniques to improve the quality of the learned feature representations. Many state-of-the-art multimodal systems leverage self-supervised or unsupervised pre-training on large-scale datasets, which has been shown to improve downstream task performance.

Finally, a deeper investigation into the impact of visibility degradation on the feature representations extracted by different network architectures is warranted. A thorough understanding of this relationship is crucial for improving the trustworthiness and reliability of these models in safety-critical, real-world applications.



