\documentclass[conf]{new-aiaa}
%\documentclass[journal]{new-aiaa} for journal papers
\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[version=4]{mhchem}
\usepackage{siunitx}
\usepackage{amssymb}
\usepackage{longtable,tabularx}
\usepackage{booktabs}

\usepackage{acro}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{subcaption}


\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{ifthen}
\usepackage{multirow}

\usetikzlibrary{matrix,calc}
% \addbibresource{bibs/ieee.bib}

\setlength\LTleft{0pt} 

\title{SeeNN: Leveraging Multimodal Deep Learning for In-Flight Long-Range Atmospheric Visibility Estimation in Aviation Safety}

\author{Taha Bouhsine \footnote{Graduate Research Fellow}, Giuseppina Carannante.\footnote{Postdoctoral Fellow}, Nidhal C. Bouaynaya.\footnote{Associate Dean for Research \& Graduate Studies and Professor of Electrical \& Computer Engineering}}
\affil{Electrical and Computer Engineering Department, Henery M.Rowan College of Engineering, Rowan University, Glassboro, New Jersey, 08028}
\author{Soufiane Idbraim \footnote{Computer Science Professor and Head of IRF-SIC Laboratory}}
\affil{IRF-SIC Laboratory, Computer Science Department, Faculty of Sciences Agadir, Ibn Zohr University, Agadir, Morocco}
\author{Phuong Tran, Grant Morfit, Maggie Mayfield, Charles Cliff Johnson}
\affil{William J. Hughes Technical Center, Federal Aviation Administration, Atlantic City, NJ, USA}

\begin{document}

\maketitle

\begin{abstract}
Deep learning (DL) models have attained state-of-the-art performance in numerous fields. Nevertheless, for certain real-world applications, existing models encounter diverse challenges, ranging from a lack of generability to new data to issues of scalability and overfitting. In this context, integrating information extracted from different modalities holds promise as a potential solution to alleviate these challenges.
This paper introduces SeeNN (\url{https://github.com/skywolfmo/seeNN-paper}), a multimodal deep-learning framework for long-range atmospheric visibility estimation. Using multimodal deep learning, SeeNN fuses various modalities to estimate long-range atmospheric visibility. These modalities include RGB imagery, Edge Map, Entropy Map, Depth Map, and Normal Surface Map. Results show that in contrast to single-modality RGB, which achieves only 87.92\% accuracy, multimodal deep learning models achieve an accuracy of over 96\%.
This significant improvement highlights the potential of multimodal approaches to enhance the accuracy and reliability of atmospheric visibility estimation, which is crucial for improving safety in applications such as aviation, maritime navigation, and autonomous vehicles. By addressing challenges such as data variability, environmental factors, and the inherent complexity of atmospheric conditions, SeeNN contributes to more reliable and robust visibility estimation systems, thereby enhancing safety and operational efficiency in critical environments.






\end{abstract}

% \section{Nomenclature}

% {\renewcommand\arraystretch{1.0}
% \noindent\begin{longtable*}{@{}l @{\quad=\quad} l@{}}
% $A$  & amplitude of oscillation \\
% $a$ &    cylinder diameter \\
% $C_p$& pressure coefficient \\
% $Cx$ & force coefficient in the \textit{x} direction \\
% $Cy$ & force coefficient in the \textit{y} direction \\
% c   & chord \\
% d$t$ & time step \\
% $Fx$ & $X$ component of the resultant pressure force acting on the vehicle \\
% $Fy$ & $Y$ component of the resultant pressure force acting on the vehicle \\
% $f, g$   & generic functions \\
% $h$  & height \\
% $i$  & time index during navigation \\
% $j$  & waypoint index \\
% $K$  & trailing-edge (TE) nondimensional angular deflection rate
% \end{longtable*}}

\input {chapters/intro}

\input{chapters/related_work}
\input{chapters/materials}
\input{chapters/results}
\input{chapters/conclusion}
\input{chapters/ack}
\bibliography{sample}




\end{document}
