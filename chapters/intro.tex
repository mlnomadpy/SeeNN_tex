\section{Introduction}
\label{sec:introduction}

Atmospheric visibility is a critical determinant of aviation safety, directly influencing a pilot's capacity for navigation and critical decision-making \cite{visibilitybook, Kulesa2003WEATHERAA, fultz2016fatal, long2022analysis, fujita1977analysis, ramee2021analysis}. The tragic 2020 accident involving Kobe Bryant, which the National Transportation Safety Board (NTSB) attributed to the pilot's decision to continue flight under Visual Flight Rules (VFR) into Instrument Meteorological Conditions (IMC), starkly underscores the severe consequences of impaired visibility. This incident highlights the urgent and unmet need for accurate, real-time, in-flight visibility estimation technologies.

The development of automated visibility estimation systems for aviation is fraught with challenges. Pilots frequently rely on their familiarity with local landmarks and terrain, a dependency that complicates the creation of automated systems requiring broad geographical adaptability \cite{ahlstrom2019assessments}. Moreover, the dynamic nature of atmospheric conditions, including fluctuating cloud cover and abrupt weather changes, necessitates solutions that are both robust and versatile.

While deep learning presents promising avenues for addressing complex problems, unimodal approaches, particularly those reliant on RGB imagery, have demonstrated significant limitations in the context of visibility estimation. These models are often susceptible to overfitting, exhibit poor generalization to new environments, and are affected by inherent biases in the training data. RGB data alone is frequently insufficient for capturing the nuanced characteristics of the atmosphere or for mitigating confounding factors such as glare, low-light conditions, or rapid meteorological shifts \cite{Bouhsine2022, AitOuadil2023, li_meteorological_2017, chaabani_estimating_2018, palvanov_dhcnn_2018, choi_automatic_2018, you_relative_2019, li_method_2019, outay_estimating_2021}.

To overcome these obstacles, multimodal deep learning has emerged as a demonstrably superior paradigm. By integrating data from diverse sources, these techniques augment the capabilities of the model and address the intrinsic shortcomings of single-modality systems \cite{liu2018learn, castanedo2013review, molino2021improved, blasch2021machine}. Each modality contributes unique information, fostering a more holistic and veridical perception of the environment, which in turn leads to more precise and reliable predictions. The value of multimodal deep learning in visibility estimation is increasingly recognized, as evidenced by a growing body of literature \cite{palvanov_visnet_2019, department_of_computer_science_chu_hai_college_of_higher_education_80_castle_peak_road_castle_peak_bay_tuen_mun_nt_hong_kong_meteorology_2020, AitOuadil2023, app11030997, Zhang2023, You2022, Chen2022, Wauben2016, Cheng2018, Zhou2021}. As detailed in \cref{tab:literature_review}, the fusion of multiple data streams enhances the robustness, safety, and reliability of deep learning systems, rendering them suitable for mission-critical real-world applications \cite{6817512, liang2024foundationsmultisensoryartificialintelligence, huang2021makesmultimodallearningbetter}.

\begin{table}[htbp]
\centering
\caption{Modalities Employed in On-Ground Atmospheric Visibility Estimation Literature}
\label{tab:literature_review}
\begin{tabular}{@{}lccccccc@{}}
\toprule
\textbf{Modality} & \textbf{\cite{You2022}} & \textbf{\cite{Palvanov2019}} & \textbf{\cite{Zhang2023}} & \textbf{\cite{Chen2022}} & \textbf{\cite{Wauben2016}} & \textbf{\cite{Cheng2018}} & \textbf{\cite{Zhou2021}} \\
\midrule
Depth Map &  & & X & & & & \\
Transmission Map  & X & & X & & X & & \\
Disparity Map & X & & & & & &  \\
Entropy & & & & &  & X & X \\
Edge Detection & & & & & X & &  \\
Contrast Computation   & & & & & X & & \\
Koschmieder Law & &  & & & X & & X \\
FFT & & X & & & & & \\
Spectral Filter & & X & & & & & \\
Dark Channel Prior & & & & X & X & & X\\
\bottomrule
\end{tabular}
\end{table}

Despite the progress in ground-based visibility estimation, a significant gap persists in addressing in-flight scenarios. This deficiency is primarily attributable to the scarcity of comprehensive in-flight visibility datasets, which are essential for training and validating deep learning models in realistic aviation contexts \cite{AitOuadil2023}. Existing datasets are often constrained to short-range, ground-level visibility and lack the necessary diversity in scenery and land cover. This limitation severely impedes the development of universally applicable and robust in-flight visibility estimation models.

This paper introduces a multimodal framework for training visibility estimation systems, with the goal of improving the accuracy, trustworthiness, and robustness of deep learning models for atmospheric visibility assessment. We demonstrate that by integrating diverse data modalities, the limitations of unimodal RGB approaches can be substantially mitigated, thereby advancing the development of versatile and reliable deep learning applications in environmentally dynamic fields. Furthermore, we address the critical dataset gap by introducing a comprehensive, publicly available dataset that captures visibility degradation across a wide range of land covers and altitudes.

The primary contributions of this work are twofold:
\begin{itemize}
    \item A meticulously curated dataset, SeeSet V1, for the benchmarking of visibility estimation, dehazing, and visibility restoration algorithms \cite{gui2023comprehensive}. This dataset, which is publicly available at \url{https://github.com/skywolfmo/seeNN-paper}, was generated using the X-Plane 11 flight simulator. It includes a wide array of images captured under diverse visibility conditions and at various altitudes, from ground level to 2,000 feet Above Ground Level (AGL). Its comprehensiveness provides a robust foundation for the training and evaluation of advanced in-flight visibility estimation and restoration techniques.
    

    \item A multimodal fusion framework for atmospheric visibility estimation. This framework is employed to train and validate deep learning models, with results demonstrating the superior accuracy of the multimodal approach when compared to single-modality RGB models.
\end{itemize}

