\section{Conclusion}

In this work, we have presented a novel multimodal approach to atmospheric visibility estimation, focusing on challenging in-flight scenarios through the application of advanced deep-learning architectures. Our primary contributions are twofold:

\begin{itemize}
    \item We propose SeeNN, a multimodal fusion framework that integrates RGB imagery with entropy maps, edge maps, depth information, and normal surface maps. Through rigorous experimentation, we demonstrate that this multimodal approach significantly outperforms single-modality baselines, including traditional RGB-based models. The superior performance of SeeNN underscores the efficacy of leveraging diverse data modalities in addressing the complex task of visibility estimation.
    \item We introduce a comprehensive, open-source benchmark dataset for atmospheric visibility estimation. This dataset is distinguished by its diversity, encompassing a wide range of altitudes, land cover types, and visibility conditions. It represents a valuable resource for the research community, enabling robust evaluation and comparison of visibility estimation algorithms.
\end{itemize}


Our empirical results indicate that the proposed multimodal deep learning framework offers substantial improvements in estimation accuracy compared to the single-modality RGB method.  The release of our benchmark dataset addresses a critical gap in the field, providing a standardized platform for algorithm development and evaluation. We anticipate that this resource will facilitate rapid progress in the domain, spurring the development of increasingly sophisticated multimodal deep learning techniques for atmospheric visibility estimation.

Future work may explore the integration of additional modalities, the application of more advanced fusion techniques, or the extension of our approach to related problems in atmospheric science. Moreover, the potential for transfer learning and domain adaptation in this context remains an open and promising avenue for investigation.

In conclusion, our work contributes to the growing body of research at the intersection of deep learning and atmospheric science, offering both methodological advancements and resources for the broader research community. As the field continues to evolve, we believe that multimodal approaches like SeeNN will play an increasingly crucial role in addressing complex environmental perception tasks, with far-reaching implications for aviation safety and beyond.

