@misc{liu2018learn,
  title={Learn to Combine Modalities in Multimodal Deep Learning},
  author={Liu, K. and Li, Y. and Xu, N. and Natarajan, P.},
  year={2018},
  note={[Online]. Available: https://arxiv.org/abs/1805.11730}
}

@article{castanedo2013review,
  title={A Review of Data Fusion Techniques},
  author={Castanedo, F. and Ursino, D. and Takama, Y.},
  journal={The Scientific World Journal},
  volume={2013},
  pages={Article ID 704504},
  year={2013},
  note={[Online]. Available: https://doi.org/10.1155/2013/704504}
}

@article{molino2021improved,
  title={Improved Accuracy in Predicting the Best Sensor Fusion Architecture for Multiple Domains},
  author={Molino-Minero-Re, E. and Aguileta, A. A. and Brena, R. F. and Garcia-Ceja, E.},
  journal={Sensors},
  volume={21},
  number={7007},
  year={2021},
  note={[Online]. Available: https://doi.org/10.3390/s21217007}
}

@article{blasch2021machine,
  title={Machine Learning/Artificial Intelligence for Sensor Data Fusion-Opportunities and Challenges},
  author={Blasch, E. and Pham, T. and Chong, C. Y. and Koch, W. and Leung, H. and Braines, D. and Abdelzaher, T.},
  journal={IEEE Aerospace and Electronic Systems Magazine},
  volume={36},
  number={7},
  pages={80--93},
  year={2021},
  month={7},
  note={[Online]. Available: https://doi.org/10.1109/MAES.2020.3049030}
}

@inproceedings{Bouhsine2022,
  title={Atmospheric Visibility Image-Based System for Instrument Meteorological Conditions Estimation: A Deep Learning Approach},
  author={Bouhsine, T. and Idbraim, S. and Bouaynaya, N. C. and Alfergani, H. and Ouadil, K. A. and Johnson, C. C.},
  booktitle={Proc. 2022 9th International Conference on Wireless Networks and Mobile Communications (WINCOM)},
  pages={1-6},
  year={2022},
  address={Rabat, Morocco},
  note={[Online]. Available: https://doi.org/10.1109/WINCOM55661.2022.9966454}
}

@article{AitOuadil2023,
  title={Atmospheric visibility estimation: a review of deep learning approach},
  author={Ait Ouadil, K. and Idbraim, S. and Bouhsine, T. and others},
  journal={Multimedia Tools and Applications},
  year={2023},
  note={[Online]. Available: https://doi.org/10.1007/s11042-023-16855-z}
}

@article{Horvath1981,
  title={Atmospheric visibility},
  author={Horvath, H.},
  journal={Atmospheric Environment (1967)},
  volume={15},
  number={10–11},
  pages={1785-1796},
  year={1981},
  note={[Online]. Available: https://doi.org/10.1016/0004-6981(81)90214-6}
}

@inproceedings{Huang2017,
  title={Densely connected convolutional networks},
  author={Huang, G. and Liu, Z. and Van Der Maaten, L. and Weinberger, K. Q.},
  booktitle={Proc. of the IEEE conference on computer vision and pattern recognition},
  pages={4700-4708},
  year={2017}
}

@article{You2022,
  title={DMRVisNet: Deep Multihead Regression Network for Pixel-Wise Visibility Estimation Under Foggy Weather},
  author={You, J. and Jia, S. and Pei, X. and Yao, D.},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  volume={23},
  number={11},
  pages={22354-22366},
  year={2022},
  month={11},
  doi={10.1109/TITS.2022.3180229}
}

@article{Palvanov2019,
  title={VisNet: Deep Convolutional Neural Networks for Forecasting Atmospheric Visibility},
  author={Palvanov, A. and Cho, Y. I.},
  journal={Sensors},
  volume={19},
  number={6},
  pages={1343},
  year={2019},
  doi={10.3390/s19061343}
}

@article{Zhang2023,
  title={Deep Quantified Visibility Estimation for Traffic Image},
  author={Zhang, F. and Yu, T. and Li, Z. and Wang, K. and Chen, Y. and Huang, Y. and Kuang, Q.},
  journal={Atmosphere},
  volume={14},
  number={1},
  pages={61},
  year={2023},
  doi={10.3390/atmos14010061}
}

@article{Chen2022,
  title={Dark Channel Based Visibility Measuring from Daytime Scene Videos},
  author={Chen, X.-H. and Li, Z.},
  journal={Journal of Internet Technology},
  volume={23},
  number={3},
  pages={593-599},
  year={2022},
  month={5}
}

@inproceedings{Wauben2016,
  title={Exploration of Fog Detection and Visibility Estimation from Camera Images},
  author={Wauben, W. and Roth, M.},
  booktitle={WMO Technical Conference on Meteorological and Environmental Instruments and Methods of Observation, CIMO TECO},
  pages={1-14},
  year={2016},
  month={9}
}

@misc{Cheng2018,
  title={Expressway Visibility Estimation Based on Image Entropy and Piecewise Stationary Time Series Analysis},
  author={Cheng, X. and Liu, G. and Hedman, A. and Wang, K. and Li, H.},
  year={2018},
  note={[Online]. Available: arXiv:1804.04601}
}

@conference{Zhou2021,
  title={Research on visibility detection model optimization based on dark channel prior and image entropy and visibility development trend prediction},
  author={Zhou, H. and Dai, M. and Shi, D. and Meng, Y. and Peng, B. and Chen, T.},
  booktitle={IOP Conference Series: Earth and Environmental Science},
  volume={826},
  number={1},
  pages={012031},
  year={2021},
  month={7},
  doi={10.1088/1755-1315/826/1/012031}
}

@misc{eftekhar2021omnidata,
  title={Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision Datasets from 3D Scans},
  author={Eftekhar, A. and Sax, A. and Bachmann, R. and Malik, J. and Zamir, A.},
  year={2021},
  note={[Online]. Available: https://arxiv.org/abs/2110.04994}
}

@article{lahat2015multimodal,
  title={Multimodal Data Fusion: An Overview of Methods, Challenges and Prospects},
  author={Lahat, D. and Adali, T. and Jutten, C.},
  journal={Proceedings of the IEEE},
  volume={103},
  number={9},
  pages={1449--1477},
  year={2015},
  month={8},
  note={[Online]. Available: https://doi.org/10.1109/JPROC.2015.2460697}
}

@article{li2017haze,
  title={Haze visibility enhancement: A Survey and quantitative benchmarking},
  author={Li, Y. and You, S. and Brown, M. S. and Tan, R. T.},
  journal={Computer Vision and Image Understanding},
  volume={165},
  pages={1-16},
  year={2017},
  note={[Online]. Available: https://doi.org/10.1016/j.cviu.2017.09.003}
}

@misc{biewald_experiment_2020,
	title = {Experiment {Tracking} with {Weights} and {Biases}},
	url = {https://www.wandb.com/},
	author = {Biewald, Lukas},
	year = {2020},
}

@misc{yang_image_2022,
	title = {Image {Data} {Augmentation} for {Deep} {Learning}: {A} {Survey}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2204.08610},
	publisher = {arXiv},
	author = {Yang, Suorong and Xiao, Weikang and Zhang, Mengcheng and Guo, Suhan and Zhao, Jian and Shen, Furao},
	year = {2022},
	doi = {10.48550/ARXIV.2204.08610},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
}

@misc{noauthor_what_nodate,
	title = {What is "{Neural} {Network}"},
	url = {https://ze-us.xyz/blog/item/390-what-is-neural-network},
	abstract = {Zeus blog. Be in touch with technologies.},
	language = {en-gb},
	urldate = {2021-04-10},
}

@misc{malanca_introduction_2019,
	title = {Introduction to {Artificial} {Neural} {Networks}},
	url = {https://adatis.co.uk/introduction-to-artificial-neural-networks/},
	abstract = {Artificial Neural Networks made easy for enyone to understand. A comprehensive guide on how ANNs work, when and where to use them.},
	language = {en-GB},
	urldate = {2021-04-10},
	journal = {Adatis},
	author = {Malanca, Alexandru},
	month = oct,
	year = {2019},
}

@misc{noauthor_classification_2019,
	title = {Classification {In} {Machine} {Learning} {\textbackslash}textbar {Classification} {Algorithms}},
	url = {https://www.edureka.co/blog/classification-in-machine-learning/},
	abstract = {This article covers the concept of classification in machine learning with classification algorithms, classifier evaluation, use cases, etc.},
	language = {en-US},
	urldate = {2021-04-10},
	journal = {Edureka},
	month = dec,
	year = {2019},
}

@misc{noauthor_junior_nodate,
	title = {Junior science-classification},
	url = {http://www.dynamicscience.com.au/tester/solutions1/biology/classification/class.html},
	urldate = {2021-04-10},
}

@misc{services_artificial_nodate,
	title = {Artificial {Neural} {Networks}},
	url = {https://dataasservices.com/artificial-neural-networks},
	abstract = {Artificial neural network system is formed by the interconnection of nodes referred to as artificial neurones, constituting structure is similar to that of animal brains.},
	language = {en},
	urldate = {2021-04-10},
	journal = {Data As Services},
	author = {Services, Data As},
}

@misc{noauthor_social_nodate,
	title = {Social {Network} for {Programmers} and {Developers}},
	url = {https://morioh.com/p/1063d43ef15e},
	abstract = {Morioh is the place to create a Great Personal Brand, connect with Developers around the World and Grow your Career!},
	language = {en},
	urldate = {2021-04-10},
}

@misc{knocklein_classification_2019,
	title = {Classification {Using} {Neural} {Networks}},
	url = {https://towardsdatascience.com/classification-using-neural-networks-b8e98f3a904f},
	abstract = {Neural networks are one of those cool words that are often used to lend credence to research. But what exactly are they? After reading…},
	language = {en},
	urldate = {2021-04-10},
	journal = {Medium},
	author = {Knocklein, Oliver},
	month = jun,
	year = {2019},
}

@misc{jayawant_introduction_2017,
	title = {Introduction to machine learning},
	url = {https://medium.com/quartifex/introduction-to-machine-learning-b2903ca088cc},
	abstract = {Let’s learn how machines learn},
	language = {en},
	urldate = {2021-04-10},
	journal = {Medium},
	author = {Jayawant, Shubham},
	month = apr,
	year = {2017},
}

@misc{tyagi_journey_2020,
	title = {The journey of a {Deep} {Neural} {Network} {Model}.},
	url = {https://medium.com/analytics-vidhya/the-journey-of-a-deep-neural-network-model-3a7697c88f7b},
	abstract = {Round 1},
	language = {en},
	urldate = {2021-04-10},
	journal = {Medium},
	author = {Tyagi, Rashu},
	month = feb,
	year = {2020},
}

@misc{paila_multilayered_2020,
	title = {Multilayered {Neural} {Network} from scratch using python},
	url = {https://medium.com/@udaybhaskarpaila/multilayered-neural-network-from-scratch-using-python-c0719a646855},
	abstract = {In this article i will tell about What is multi layered neural network and how to build multi layered neural network from scratch using…},
	language = {en},
	urldate = {2021-04-10},
	journal = {Medium},
	author = {Paila, Uday},
	month = jan,
	year = {2020},
}

@misc{ognjanovski_everything_2020,
	title = {Everything you need to know about {Neural} {Networks} and {Backpropagation} — {Machine} {Learning} {Made} {Easy}…},
	url = {https://towardsdatascience.com/everything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a},
	abstract = {Neural Network explanation from the ground including understanding the math behind it},
	language = {en},
	urldate = {2021-04-10},
	journal = {Medium},
	author = {Ognjanovski, Gavril},
	month = jun,
	year = {2020},
}

@misc{sharma_activation_2019,
	title = {Activation {Functions} in {Neural} {Networks}},
	url = {https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6},
	abstract = {Sigmoid, tanh, Softmax, ReLU, Leaky ReLU EXPLAINED ‼!},
	language = {en},
	urldate = {2021-04-10},
	journal = {Medium},
	author = {SHARMA, SAGAR},
	month = feb,
	year = {2019},
}

@misc{noauthor_convolutional_nodate,
	title = {Convolutional {Neural} {Networks} ({CNN}): {Step} 1(b) - {ReLU} {Layer} - {Blogs} {SuperDataScience} - {Machine} {Learning} {\textbackslash}textbar {AI} {\textbackslash}textbar {Data} {Science} {Career} {\textbackslash}textbar {Analytics} {\textbackslash}textbar {Success}},
	url = {https://www.superdatascience.com/blogs/convolutional-neural-networks-cnn-step-1b-relu-layer},
	urldate = {2021-04-10},
}

@misc{zerium_demystifying_2018,
	title = {Demystifying {Convolutional} {Neural} {Networks}},
	url = {https://medium.com/@eternalzer0dayx/demystifying-convolutional-neural-networks-ca17bdc75559},
	abstract = {An Intuitive Explanation of Convolutional Neural Networks.},
	language = {en},
	urldate = {2021-04-10},
	journal = {Medium},
	author = {Zerium, Aegeus},
	month = sep,
	year = {2018},
}

@misc{saha_comprehensive_2018,
	title = {A {Comprehensive} {Guide} to {Convolutional} {Neural} {Networks} — the {ELI5} way},
	url = {https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53},
	abstract = {Artificial Intelligence has been witnessing a monumental growth in bridging the gap between the capabilities of humans and machines…},
	language = {en},
	urldate = {2021-04-10},
	journal = {Medium},
	author = {Saha, Sumit},
	month = dec,
	year = {2018},
}

@misc{hue_dense_2020,
	title = {Dense or {Convolutional} — {Part}-1},
	url = {https://medium.com/analytics-vidhya/dense-or-convolutional-part-1-c75c59c5b4ad},
	abstract = {When it comes to designing a deep neural network (DNN), there are a few top-level architecture choices, one of them is should I use a…},
	language = {en},
	urldate = {2021-04-10},
	journal = {Medium},
	author = {Hue, Antoine},
	month = jul,
	year = {2020},
}

@misc{noauthor_pdf_nodate,
	title = {({PDF}) {Semantic} {Segmentation} for {Aerial} {Mapping}},
	url = {https://www.researchgate.net/publication/343991584_Semantic_Segmentation_for_Aerial_Mapping},
	abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
	language = {en},
	urldate = {2021-04-10},
	journal = {ResearchGate},
}

@article{jaswal_image_2014,
	title = {Image {Classification} {Using} {Convolutional} {Neural} {Networks}},
	volume = {3},
	abstract = {Deep Learning has emerged as a new area in machine learning and is applied to a number of signal and image applications.The main purpose of the work presented in this paper, is to apply the concept of a Deep Learning algorithm namely, Convolutional neural networks (CNN) in image classification. The algorithm is tested on various standard datasets, like remot e sensing data of aerial images (UC Merced Land Use Dataset) and scene images from SUN database. The performance of the algorithm is evaluated based on the quality metric known as Mean Squared Error (MSE) and classification accuracy. The graphical representation of the experimental results is given on the basis of MSE against the number of training epochs. The experimental result analysis based on the quality metrics and the graphical representation proves that the algorithm (CNN) gives fairly good classification accuracy for all the tested datasets.},
	language = {en},
	number = {6},
	author = {Jaswal, Deepika},
	year = {2014},
	pages = {8},
}

@misc{noauthor_deep_2016,
	title = {Deep {Learning} {Neural} {Networks} {Simplified}},
	url = {http://www.cognitivetoday.com/2016/10/deep-learning-neural-networks-simplified.html},
	abstract = {Deep learning is not as complex a concept that non-science people often happen to decipher. Scientific evolution over the years have reached a stage where a lot of explorations and defined research work needs the assistance of artificial intelligence. Since machines are usually fed with a par…},
	language = {en},
	urldate = {2021-04-10},
	journal = {Cognitive Today :The New World of Advanced Analytics and Artificial Intelligence},
	month = oct,
	year = {2016},
}

@misc{noauthor_using_2020,
	title = {Using model.predict() with your {TensorFlow} / {Keras} model},
	url = {https://www.machinecurve.com/index.php/2020/02/21/how-to-predict-new-samples-with-your-keras-model/},
	abstract = {Model.predict in TensorFlow and Keras can be used for predicting new samples. Learn how, with step-by-step explanations and code examples.},
	language = {en-US},
	urldate = {2021-04-10},
	journal = {MachineCurve},
	month = feb,
	year = {2020},
}

@misc{veen_neural_2016,
	title = {The {Neural} {Network} {Zoo}},
	url = {https://www.asimovinstitute.org/neural-network-zoo/},
	abstract = {With new neural network architectures popping up every now and then, it’s hard to keep track of them all. Knowing all the abbreviations being thrown around (DCIGN, BiLSTM, DCGAN, anyone?) can be a bit overwhelming at first. So I decided to compose a cheat sheet containing many of those architectures. Most of these are neural networks, some are completely […]},
	language = {en-US},
	urldate = {2021-04-10},
	journal = {The Asimov Institute},
	author = {Veen, Fjodor van},
	month = sep,
	year = {2016},
}

@misc{noauthor_reinforcement_nodate,
	title = {Reinforcement learning · {RL} @illya13},
	url = {https://illya13.github.io/RL/reinforcement-learning.html},
	urldate = {2021-04-10},
}

@misc{sharma_keras_2020,
	title = {Keras {Dense} {Layer} {Explained} for {Beginners}},
	url = {https://machinelearningknowledge.ai/keras-dense-layer-explained-for-beginners/},
	abstract = {In this article, we will go through tutorial of Keras Dense Layer function where will explain syntax along with examples.},
	language = {en-US},
	urldate = {2021-04-10},
	journal = {MLK - Machine Learning Knowledge},
	author = {Sharma, Palash},
	month = oct,
	year = {2020},
}

@article{dosovitskiy_image_2020,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	volume = {abs/2010.11929},
	url = {https://arxiv.org/abs/2010.11929},
	journal = {CoRR},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	year = {2020},
	note = {arXiv: 2010.11929},
}

@article{tarel_vision_2012,
	title = {Vision {Enhancement} in {Homogeneous} and {Heterogeneous} {Fog}},
	volume = {4},
	number = {2},
	journal = {IEEE Intelligent Transportation Systems Magazine},
	author = {Tarel, J.-P. and Hautière, N. and Caraffa, L. and Cord, A. and Halmaoui, H. and Gruyer, D.},
	year = {2012},
	note = {Publisher: IEEE},
	pages = {6--20},
}

@inproceedings{tarel_improved_nodate,
	address = {San Diego, California, USA},
	title = {Improved {Visibility} of {Road} {Scene} {Images} under {Heterogeneous} {Fog}},
	booktitle = {Proceedings of {IEEE} {Intelligent} {Vehicle} {Symposium} ({IV}'2010)},
	author = {Tarel, J.-P. and Hautière, N. and Cord, A. and Gruyer, D. and Halmaoui, H.},
	month = jun,
	pages = {478--485},
}

@article{li_benchmarking_2019,
	title = {Benchmarking {Single}-{Image} {Dehazing} and {Beyond}},
	volume = {28},
	number = {1},
	journal = {IEEE Transactions on Image Processing},
	author = {Li, Boyi and Ren, Wenqi and Fu, Dengpan and Tao, Dacheng and Feng, Dan and Zeng, Wenjun and Wang, Zhangyang},
	year = {2019},
	note = {Publisher: IEEE},
	pages = {492--505},
}

@article{zoph_learning_2017,
	title = {Learning {Transferable} {Architectures} for {Scalable} {Image} {Recognition}},
	volume = {abs/1707.07012},
	url = {http://arxiv.org/abs/1707.07012},
	journal = {CoRR},
	author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
	year = {2017},
	note = {arXiv: 1707.07012},
}

@article{van_pelt_dust_2020,
	title = {Dust emission source characterization for visibility hazard assessment on {Lordsburg} {Playa} in {Southwestern} {New} {Mexico}, {USA}},
	volume = {7},
	issn = {2197-8670},
	url = {https://doi.org/10.1186/s40677-020-00171-x},
	doi = {10.1186/s40677-020-00171-x},
	abstract = {In drylands around the world, ephemeral lakes (playas) are common. Dry, wind-erodible playa sediments are potent local and regional sources of dust and PM10 (airborne particles with diameters less than 10 μm). Dust clouds often cause sudden and/or prolonged loss of visibility to travelers on downwind roadways. Lordsburg Playa, in southwestern New Mexico, USA is bisected by Interstate Highway 10. Dust storms emanating from the playa have been responsible for numerous visibility-related road closures (including 39 road closures between 2012 and 2019) causing major economic losses, in addition to well over a hundred dust-related vehicle crashes causing at least 41 lost lives in the last 53 years. In order to improve understanding of the surfaces responsible for the dust emissions, we investigated the critical wind friction velocity thresholds and the dust emissivities of surfaces representing areas typical of Lordsburg Playa’s stream deltas, shorelines, and ephemerally flooded lakebed using a Portable In-Situ Wind ERosion Laboratory (PI-SWERL). Mean threshold friction velocities for PM10 entrainment ranged from less than 0.30 m s− 1 for areas in the delta and shoreline to greater than 0.55 m s− 1 for ephemerally flooded areas of the lakebed. Similarly, we quantified mean PM10 vertical flux rates ranging from less than 500 μg m− 2 s− 1 for ephemerally flooded areas of lakebed to nearly 25,000 μg m− 2 s− 1 for disturbed delta surfaces. The unlimited PM10 supply of the relatively coarse sediments along the western shoreline is problematic and indicates that this may be the source area for longer-term visibility reducing dust events and should be a focus area for dust mitigation efforts.},
	number = {1},
	urldate = {2022-06-28},
	journal = {Geoenvironmental Disasters},
	author = {Van Pelt, R. Scott and Tatarko, John and Gill, Thomas E. and Chang, Chunping and Li, Junran and Eibedingil, Iyasu G. and Mendez, Marcos},
	month = dec,
	year = {2020},
	keywords = {Dust storms, Highway safety, PM10, Surface emissivity, Visibility, Wind erosion},
	pages = {34},
	file = {Full Text PDF:/home/skywolfmo/Zotero/storage/CXJCZU6A/Van Pelt et al. - 2020 - Dust emission source characterization for visibili.pdf:application/pdf;Snapshot:/home/skywolfmo/Zotero/storage/DUKYR7TA/s40677-020-00171-x.html:text/html},
}

@article{ngo_visibility_2021,
	title = {Visibility {Restoration}: {A} {Systematic} {Review} and {Meta}-{Analysis}},
	volume = {21},
	issn = {1424-8220},
	shorttitle = {Visibility {Restoration}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8069147/},
	doi = {10.3390/s21082625},
	abstract = {Image acquisition is a complex process that is affected by a wide variety of internal and environmental factors. Hence, visibility restoration is crucial for many high-level applications in photography and computer vision. This paper provides a systematic review and meta-analysis of visibility restoration algorithms with a focus on those that are pertinent to poor weather conditions. This paper starts with an introduction to optical image formation and then provides a comprehensive description of existing algorithms as well as a comparative evaluation. Subsequently, there is a thorough discussion on current difficulties that are worthy of a scientific effort. Moreover, this paper proposes a general framework for visibility restoration in hazy weather conditions while using haze-relevant features and maximum likelihood estimates. Finally, a discussion on the findings and future developments concludes this paper.},
	number = {8},
	urldate = {2022-06-28},
	journal = {Sensors (Basel, Switzerland)},
	author = {Ngo, Dat and Lee, Seungmin and Ngo, Tri Minh and Lee, Gi-Dong and Kang, Bongsoon},
	month = apr,
	year = {2021},
	pmid = {33918021},
	pmcid = {PMC8069147},
	pages = {2625},
	file = {Full Text:/home/skywolfmo/Zotero/storage/R2MRJUP7/Ngo et al. - 2021 - Visibility Restoration A Systematic Review and Me.pdf:application/pdf},
}

@misc{noauthor_bouguer_nodate,
	title = {Bouguer, {Lambert}, and the {Theory} of {Horizontal} {Visibility} on {JSTOR}},
	url = {https://www.jstor.org/stable/226845},
	urldate = {2022-06-28},
	file = {Bouguer, Lambert, and the Theory of Horizontal Visibility on JSTOR:/home/skywolfmo/Zotero/storage/4Q6XGKKE/226845.html:text/html},
}

@article{van_beelen_bachelor_nodate,
	title = {Bachelor thesis {Utrecht} {University} {Institute} for {Marine} and {Atmospheric} {Research} {Utrecht} http://www.imau.nl/ {June} 2010},
	language = {en},
	author = {van Beelen, Aldert J},
	pages = {38},
	file = {van Beelen - Bachelor thesis Utrecht University Institute for M.pdf:/home/skywolfmo/Zotero/storage/SDVR3WKA/van Beelen - Bachelor thesis Utrecht University Institute for M.pdf:application/pdf},
}

@article{noauthor_estimating_nodate,
	title = {Estimating {Inflight} {Visibility}},
	language = {en},
	pages = {4},
	file = {Estimating Inflight Visibility.pdf:/home/skywolfmo/Zotero/storage/QY323NA9/Estimating Inflight Visibility.pdf:application/pdf},
}

@techreport{coakley_estimates_1948,
	title = {{ESTIMATES} {OF} {VISIBILITY} {FROM} {HIGH} {ALTITUDE} {AIRCRAFT},},
	url = {https://apps.dtic.mil/sti/citations/AD0642798},
	abstract = {The distance at which an object becomes visible depends upon such factors as brightness contrast, target size, brightness level, and atmospheric attenuation. Numerical values are selected to represent the influence of these variables under the conditions of high altitude flight. Visibility is examined under the following conditions a Altitude of observer–sea level, 50,000, 100,000, and 200,000 feet. b Altitude of target–from sea level to approximately 300,000 feet 60 miles. c Background brightness conditions–from 1 to 32 footlamberts. d Contrast ratio between target and background–for values of 1, 5, 10, and 100. The computation of visibility is based on visual acuity data corrected for the effect of atmospheric attenuation at various altitudes. A round object with a diameter of 10 feet may be used as a representative target. Author},
	language = {en},
	urldate = {2022-06-28},
	institution = {PSYCHOLOGICAL CORP NEW YORK},
	author = {Coakley, John D.},
	month = apr,
	year = {1948},
	file = {Snapshot:/home/skywolfmo/Zotero/storage/JMUDFAWY/AD0642798.html:text/html},
}

@article{jha_high_2012,
	title = {High {Altitude} and the {Eye}},
	volume = {1},
	issn = {2162-0989},
	doi = {10.1097/APO.0b013e318253004e},
	abstract = {The purpose of this study was to review the available data on the effect of high altitude on the eyes. We carried out electronic literature search on www.pubmed.com for articles published through year 2011. The search terms included high altitude and the eye, high-altitude retinopathy, eye problems in the Himalayas, and eye diseases in Tibet. Other terms like visual functions, intraocular pressure, corneal thickness, tear function, and ocular motility, at high altitude, were searched separately and in combination. Data were retrieved from both prospective and retrospective studies published in the English language.High altitude has both short-term and long-term effects on the eyes. The short-term effects include high-altitude retinopathy, change in corneal thickness, and photokeratitis. Long-term effects include pterygium, cataract, and dry eye syndrome. High-altitude retinopathy of mild degree does not affect vision but has a predictive value for the development of high-altitude cerebral edema. Change in corneal thickness at altitude induces refractive changes in eyes with radial keratotomy and in eyes with LASIK. High altitude does not adversely affect visual acuity and contrast sensitivity; scotopic vision may be affected if supplemental oxygen is not used.},
	language = {eng},
	number = {3},
	journal = {Asia-Pacific Journal of Ophthalmology (Philadelphia, Pa.)},
	author = {Jha, Kirti Nath},
	month = jun,
	year = {2012},
	pmid = {26107334},
	pages = {166--169},
}

@misc{noauthor_learning_nodate,
	title = {Learning {Center} {Library} {Contents} - {FAA} - {FAASTeam} - {FAASafety}.gov},
	url = {https://www.faasafety.gov/gslac/alc/libview_printerfriendly.aspx?id=6851},
	urldate = {2022-06-28},
	file = {Learning Center Library Contents - FAA - FAASTeam - FAASafety.gov:/home/skywolfmo/Zotero/storage/B4TAM2WZ/libview_printerfriendly.html:text/html},
}

@misc{noauthor_11_2013,
	title = {11. {Flight} visibility and vision at high altitudes},
	url = {https://flightcrewguide.com/wiki/meteorology/visibility/11-flight-visibility-vision-high-altitudes/},
	abstract = {Flight visibility and vision at high altitudes Flight visibility is frequently different from the surface visibility. Such differences are caused by the unequal distribution of the obscuring partic…},
	language = {en-US},
	urldate = {2022-06-28},
	month = nov,
	year = {2013},
	note = {Publication Title: Flight Crew Guide},
	file = {Snapshot:/home/skywolfmo/Zotero/storage/P8NUY88F/11-flight-visibility-vision-high-altitudes.html:text/html},
}

@article{sakaridis_semantic_2018,
	title = {Semantic {Foggy} {Scene} {Understanding} with {Synthetic} {Data}},
	volume = {126},
	url = {https://doi.org/10.1007/s11263-018-1072-8},
	number = {9},
	journal = {International Journal of Computer Vision},
	author = {Sakaridis, Christos and Dai, Dengxin and Van Gool, Luc},
	month = sep,
	year = {2018},
	pages = {973--992},
}

@misc{team_keras_nodate,
	title = {Keras documentation: {Image} classification with {Vision} {Transformer}},
	shorttitle = {Keras documentation},
	url = {https://keras.io/examples/vision/image_classification_with_vision_transformer/#introduction},
	abstract = {Keras documentation},
	language = {en},
	urldate = {2022-06-28},
	author = {Team, Keras},
	file = {Snapshot:/home/skywolfmo/Zotero/storage/NP3R5UJA/image_classification_with_vision_transformer.html:text/html},
}

@misc{noauthor_weights_nodate,
	title = {Weights \& {Biases}},
	url = {https://wandb.ai/ayush-thakur/interpretability/reports/Interpretability-in-Deep-Learning-With-W-B-CAM-and-GradCAM--Vmlldzo5MTIyNw},
	abstract = {Weights \& Biases, developer tools for machine learning},
	language = {en},
	urldate = {2022-06-28},
	note = {Publication Title: W\&B},
}

@article{miclea_visibility_2021,
	title = {Visibility {Enhancement} and {Fog} {Detection}: {Solutions} {Presented} in {Recent} {Scientific} {Papers} with {Potential} for {Application} to {Mobile} {Systems}},
	volume = {21},
	issn = {1424-8220},
	shorttitle = {Visibility {Enhancement} and {Fog} {Detection}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8150865/},
	doi = {10.3390/s21103370},
	abstract = {In mobile systems, fog, rain, snow, haze, and sun glare are natural phenomena that can be very dangerous for drivers. In addition to the visibility problem, the driver must face also the choice of speed while driving. The main effects of fog are a decrease in contrast and a fade of color. Rain and snow cause also high perturbation for the driver while glare caused by the sun or by other traffic participants can be very dangerous even for a short period. In the field of autonomous vehicles, visibility is of the utmost importance. To solve this problem, different researchers have approached and offered varied solutions and methods. It is useful to focus on what has been presented in the scientific literature over the past ten years relative to these concerns. This synthesis and technological evolution in the field of sensors, in the field of communications, in data processing, can be the basis of new possibilities for approaching the problems. This paper summarizes the methods and systems found and considered relevant, which estimate or even improve visibility in adverse weather conditions. Searching in the scientific literature, in the last few years, for the preoccupations of the researchers for avoiding the problems of the mobile systems caused by the environmental factors, we found that the fog phenomenon is the most dangerous. Our focus is on the fog phenomenon, and here, we present published research about methods based on image processing, optical power measurement, systems of sensors, etc.},
	number = {10},
	urldate = {2022-06-28},
	journal = {Sensors (Basel, Switzerland)},
	author = {Miclea, Răzvan-Cătălin and Ungureanu, Vlad-Ilie and Sandru, Florin-Daniel and Silea, Ioan},
	month = may,
	year = {2021},
	pmid = {34066176},
	pmcid = {PMC8150865},
	pages = {3370},
	file = {Full Text:/home/skywolfmo/Zotero/storage/NHQHAQRZ/Miclea et al. - 2021 - Visibility Enhancement and Fog Detection Solution.pdf:application/pdf},
}

@misc{noauthor_differences_nodate,
	title = {The {Differences} between {Fog} and {Dust}-{Haze} - {Disaster} {Risk} {Reduction} {Knowledge} {Service}},
	url = {https://drr.ikcest.org/tutorial/p1c48},
	urldate = {2022-06-28},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	language = {en},
	number = {7553},
	urldate = {2022-06-28},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	keywords = {Computer science, Mathematics and computing},
	pages = {436--444},
	file = {Snapshot:/home/skywolfmo/Zotero/storage/9VYSFVW9/nature14539.html:text/html},
}

@inproceedings{liaw_atmospheric_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Atmospheric {Visibility} {Monitoring} {Using} {Digital} {Image} {Analysis} {Techniques}},
	isbn = {978-3-642-03767-2},
	doi = {10.1007/978-3-642-03767-2_146},
	abstract = {Atmospheric visibility is a standard of human visual perception of the environment. It is also directly associated with air quality, polluted species and climate. The influence of urban atmospheric visibility affects not only human health but also traffic safety and human life quality. Visibility is traditionally defined as the maximum distance at which a selected target can be recognized. To replace the traditional measurement for atmospheric visibility, digital image processing schemes provide good visibility data, established by numerical index. The performance of these techniques is defined by the correlation between the observed visual range and the obtained index. Since performance is affected by non-uniform illumination, this paper proposes a new procedure to estimate the visibility index with a sharpening method. The experimental results show that the proposed procedure obtains a better correlation coefficient than previous schemes.},
	language = {en},
	booktitle = {Computer {Analysis} of {Images} and {Patterns}},
	publisher = {Springer},
	author = {Liaw, Jiun-Jian and Lian, Ssu-Bin and Huang, Yung-Fa and Chen, Rung-Ching},
	editor = {Jiang, Xiaoyi and Petkov, Nicolai},
	year = {2009},
	keywords = {atmospheric visibility, digital image processing, homomorphic filtering},
	pages = {1204--1211},
}

@misc{noauthor_optimizers_nodate,
	title = {Optimizers — {ML} {Glossary} documentation},
	url = {https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html},
	urldate = {2022-06-28},
}

@misc{noauthor_imbalanced_nodate,
	title = {Imbalanced {Data} {\textbackslash}textbar {Data} {Preparation} and {Feature} {Engineering} for {Machine} {Learning}},
	url = {https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data},
	language = {en},
	urldate = {2022-06-28},
	note = {Publication Title: Google Developers},
	file = {Snapshot:/home/skywolfmo/Zotero/storage/Y6NJBNTC/imbalanced-data.html:text/html},
}

@misc{noauthor_machine_nodate,
	title = {Machine {Learning} {Glossary} {\textbackslash}textbar {Google} {Developers}},
	url = {https://developers.google.com/machine-learning/glossary},
	abstract = {Compilation of key machine-learning and TensorFlow terms, with beginner-friendly definitions.},
	language = {en},
	urldate = {2022-06-28},
	file = {Snapshot:/home/skywolfmo/Zotero/storage/FE76MUTC/glossary.html:text/html},
}

@misc{noauthor_application_nodate,
	title = {The {Application} of {Deep} {Learning} in {Airport} {Visibility} {Forecast}},
	url = {https://www.scirp.org/journal/paperinformation.aspx?paperid=77877},
	urldate = {2022-06-28},
}

@article{palvanov_visnet_2019,
	title = {{VisNet}: {Deep} {Convolutional} {Neural} {Networks} for {Forecasting} {Atmospheric} {Visibility}},
	volume = {19},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	shorttitle = {{VisNet}},
	url = {https://www.mdpi.com/1424-8220/19/6/1343},
	doi = {10.3390/s19061343},
	abstract = {Visibility is a complex phenomenon inspired by emissions and air pollutants or by factors, including sunlight, humidity, temperature, and time, which decrease the clarity of what is visible through the atmosphere. This paper provides a detailed overview of the state-of-the-art contributions in relation to visibility estimation under various foggy weather conditions. We propose VisNet, which is a new approach based on deep integrated convolutional neural networks for the estimation of visibility distances from camera imagery. The implemented network uses three streams of deep integrated convolutional neural networks, which are connected in parallel. In addition, we have collected the largest dataset with three million outdoor images and exact visibility values for this study. To evaluate the model’s performance fairly and objectively, the model is trained on three image datasets with different visibility ranges, each with a different number of classes. Moreover, our proposed model, VisNet, evaluated under dissimilar fog density scenarios, uses a diverse set of images. Prior to feeding the network, each input image is filtered in the frequency domain to remove low-level features, and a spectral filter is applied to each input for the extraction of low-contrast regions. Compared to the previous methods, our approach achieves the highest performance in terms of classification based on three different datasets. Furthermore, our VisNet considerably outperforms not only the classical methods, but also state-of-the-art models of visibility estimation.},
	language = {en},
	number = {6},
	urldate = {2022-06-28},
	journal = {Sensors},
	author = {Palvanov, Akmaljon and Cho, Young Im},
	month = jan,
	year = {2019},
	keywords = {convolutional neural networks, Fast Fourier transform, spectral filter, visibility, VisNet},
	pages = {1343},
	file = {Full Text PDF:/home/skywolfmo/Zotero/storage/NLX53PJR/Palvanov and Cho - 2019 - VisNet Deep Convolutional Neural Networks for For.pdf:application/pdf;Snapshot:/home/skywolfmo/Zotero/storage/5MKYC96E/1343.html:text/html},
}

@article{fawcett_introduction_2006,
	series = {{ROC} {Analysis} in {Pattern} {Recognition}},
	title = {An introduction to {ROC} analysis},
	volume = {27},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S016786550500303X},
	doi = {10.1016/j.patrec.2005.10.010},
	abstract = {Receiver operating characteristics (ROC) graphs are useful for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research.},
	language = {en},
	number = {8},
	urldate = {2022-06-28},
	journal = {Pattern Recognition Letters},
	author = {Fawcett, Tom},
	month = jun,
	year = {2006},
	keywords = {Classifier evaluation, Evaluation metrics, ROC analysis},
	pages = {861--874},
	file = {ScienceDirect Snapshot:/home/skywolfmo/Zotero/storage/G7JP99M7/S016786550500303X.html:text/html},
}

@misc{huang_densely_2018,
	title = {Densely {Connected} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1608.06993},
	abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
	urldate = {2022-06-28},
	publisher = {arXiv},
	author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
	month = jan,
	year = {2018},
	doi = {10.48550/arXiv.1608.06993},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/skywolfmo/Zotero/storage/EKALSQ9A/Huang et al. - 2018 - Densely Connected Convolutional Networks.pdf:application/pdf;arXiv.org Snapshot:/home/skywolfmo/Zotero/storage/9ZKIATEP/1608.html:text/html},
}

@misc{noauthor_gradient-based_nodate,
	title = {Gradient-based learning applied to document recognition {\textbackslash}textbar {IEEE} {Journals} \& {Magazine} {\textbackslash}textbar {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/726791},
	urldate = {2022-06-28},
}

@article{han_pre-trained_2021,
	title = {Pre-trained models: {Past}, present and future},
	volume = {2},
	issn = {2666-6510},
	shorttitle = {Pre-trained models},
	url = {https://www.sciencedirect.com/science/article/pii/S2666651021000231},
	doi = {10.1016/j.aiopen.2021.08.002},
	abstract = {Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge model parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of downstream tasks, which has been extensively demonstrated via experimental verification and empirical analysis. It is now the consensus of the AI community to adopt PTMs as backbone for downstream tasks rather than learning models from scratch. In this paper, we take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum. Further, we comprehensively review the latest breakthroughs of PTMs. These breakthroughs are driven by the surge of computational power and the increasing availability of data, towards four important directions: designing effective architectures, utilizing rich contexts, improving computational efficiency, and conducting interpretation and theoretical analysis. Finally, we discuss a series of open problems and research directions of PTMs, and hope our view can inspire and advance the future study of PTMs.},
	language = {en},
	urldate = {2022-06-28},
	journal = {AI Open},
	author = {Han, Xu and Zhang, Zhengyan and Ding, Ning and Gu, Yuxian and Liu, Xiao and Huo, Yuqi and Qiu, Jiezhong and Yao, Yuan and Zhang, Ao and Zhang, Liang and Han, Wentao and Huang, Minlie and Jin, Qin and Lan, Yanyan and Liu, Yang and Liu, Zhiyuan and Lu, Zhiwu and Qiu, Xipeng and Song, Ruihua and Tang, Jie and Wen, Ji-Rong and Yuan, Jinhui and Zhao, Wayne Xin and Zhu, Jun},
	month = jan,
	year = {2021},
	keywords = {Artificial intelligence, Language models, Multimodal processing, Natural language processing, Pre-trained models, Self-supervised learning, Transfer learning},
	pages = {225--250},
	file = {ScienceDirect Snapshot:/home/skywolfmo/Zotero/storage/R8KT7WPP/S2666651021000231.html:text/html;Submitted Version:/home/skywolfmo/Zotero/storage/GH837V34/Han et al. - 2021 - Pre-trained models Past, present and future.pdf:application/pdf},
}

@incollection{thrun_learning_1998,
	address = {Boston, MA},
	title = {Learning to {Learn}: {Introduction} and {Overview}},
	isbn = {978-1-4615-5529-2},
	shorttitle = {Learning to {Learn}},
	url = {https://doi.org/10.1007/978-1-4615-5529-2_1},
	abstract = {Over the past three decades or so, research on machine learning and data mining has led to a wide variety of algorithms that learn general functions from experience. As machine learning is maturing, it has begun to make the successful transition from academic research to various practical applications. Generic techniques such as decision trees and artificial neural networks, for example, are now being used in various commercial and industrial applications (see e.g., [Langley, 1992; Widrow et al., 1994]).},
	language = {en},
	urldate = {2022-06-28},
	booktitle = {Learning to {Learn}},
	publisher = {Springer US},
	author = {Thrun, Sebastian and Pratt, Lorien},
	editor = {Thrun, Sebastian and Pratt, Lorien},
	year = {1998},
	doi = {10.1007/978-1-4615-5529-2_1},
	keywords = {Face Recognition, Learning Algorithm, Learning Task, Reinforcement Learning, Target Function},
	pages = {3--17},
}

@misc{noauthor_transfer_nodate,
	title = {Transfer learning and fine-tuning {\textbackslash}textbar {TensorFlow} {Core}},
	url = {https://www.tensorflow.org/tutorials/images/transfer_learning},
	language = {en},
	urldate = {2022-06-28},
	note = {Publication Title: TensorFlow},
	file = {Snapshot:/home/skywolfmo/Zotero/storage/KFFDPECG/transfer_learning.html:text/html},
}

@article{dong_adaptive_2011,
	title = {Adaptive {Object} {Detection} and {Visibility} {Improvement} in {Foggy} {Image}},
	volume = {6},
	doi = {10.4304/jmm.6.1.14-21},
	abstract = {Detecting objects of interest and obtaining their clear visual appearances are critical requirements for visual surveillance systems. In this paper we propose a novel algorithm to detect foreground objects from video sequences with fog and then enhance their visibilities. First, we propose a novel metric to measure the image fog property to decide whether the image scene is obscured by fog or not. Second, if there is heavy fog in the scene, a novel approach for object detection based on an atmospheric scattering model is proposed. This novel approach can be used to detect not only newly entering objects but also sojourned objects. Once the foreground object is detected, we enhance its visibility only to avoid processing the whole image. Our proposed algorithm is tested with some surveillance video under different fog conditions. Experimental results show that the proposed approach is efficient and efficient for foreground object detection and visibility enhancement under fog weather conditions.},
	journal = {Journal of Multimedia},
	author = {Dong, Nan and Jia, Zhen and Shao, Jie and Li, Zhipeng and Liu, Fuqiang and Zhao, Jianwei and Peng, pei-yuan},
	month = feb,
	year = {2011},
	pages = {14--21},
	file = {Full Text PDF:/home/skywolfmo/Zotero/storage/NJFEWAP6/Dong et al. - 2011 - Adaptive Object Detection and Visibility Improveme.pdf:application/pdf},
}

@misc{lee_fifo_2022,
	title = {{FIFO}: {Learning} {Fog}-invariant {Features} for {Foggy} {Scene} {Segmentation}},
	shorttitle = {{FIFO}},
	url = {http://arxiv.org/abs/2204.01587},
	abstract = {Robust visual recognition under adverse weather conditions is of great importance in real-world applications. In this context, we propose a new method for learning semantic segmentation models robust against fog. Its key idea is to consider the fog condition of an image as its style and close the gap between images with different fog conditions in neural style spaces of a segmentation model. In particular, since the neural style of an image is in general affected by other factors as well as fog, we introduce a fog-pass filter module that learns to extract a fog-relevant factor from the style. Optimizing the fog-pass filter and the segmentation model alternately gradually closes the style gap between different fog conditions and allows to learn fog-invariant features in consequence. Our method substantially outperforms previous work on three real foggy image datasets. Moreover, it improves performance on both foggy and clear weather images, while existing methods often degrade performance on clear scenes.},
	urldate = {2022-06-29},
	publisher = {arXiv},
	author = {Lee, Sohyun and Son, Taeyoung and Kwak, Suha},
	month = apr,
	year = {2022},
	doi = {10.48550/arXiv.2204.01587},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/skywolfmo/Zotero/storage/GMBQVGVW/Lee et al. - 2022 - FIFO Learning Fog-invariant Features for Foggy Sc.pdf:application/pdf;arXiv.org Snapshot:/home/skywolfmo/Zotero/storage/WUZ7E4L8/2204.html:text/html},
}

@article{wauben_exploration_nodate,
	title = {{EXPLORATION} {OF} {FOG} {DETECTION} {AND} {VISIBILITY} {ESTIMATION} {FROM} {CAMERA} {IMAGES}},
	abstract = {In this report several methods to determine the presence of fog or to estimate the visibility from camera images during daytime are implemented and evaluated. The landmark discrimination method by using edge detection and visibility estimation from contrast attenuation require selection of objects at known distances. Results of the landmark discrimination method during fog conditions are good, but are affected by inhomogeneous visibility conditions, which can be identified as such. The contrast reduction method is very sensitive to the availability of suitable objects. The visibility range covered by these methods is limited by the distances of the available objects. Global image features can also be used to obtain information on visibility. A decision tree using the mean number of edges and the variation of the horizontal averages of the transmission has been constructed to determine the presence of dense fog. A linear regression model including also the mean brightness of an image has been considered to estimate the visibility. Both these methods show promising results. However, further refinement of the methods and more extensive validation against other image data sets is required.},
	language = {en},
	author = {Wauben, Wiel and Roth, Martin and Observations, D and Box, P O and Bilt, AE De},
	pages = {14},
	file = {Wauben et al. - EXPLORATION OF FOG DETECTION AND VISIBILITY ESTIMA.pdf:/home/skywolfmo/Zotero/storage/RKLEQZ7X/Wauben et al. - EXPLORATION OF FOG DETECTION AND VISIBILITY ESTIMA.pdf:application/pdf},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
}

@misc{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2022-06-29},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	doi = {10.48550/arXiv.1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/skywolfmo/Zotero/storage/AX9DU4J8/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/home/skywolfmo/Zotero/storage/WPFJFIRB/1512.html:text/html},
}

@article{simonyan_very_2014,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively.},
	journal = {arXiv 1409.1556},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = sep,
	year = {2014},
}

@misc{szegedy_going_2014,
	title = {Going {Deeper} with {Convolutions}},
	url = {http://arxiv.org/abs/1409.4842},
	abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	urldate = {2022-06-29},
	publisher = {arXiv},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	month = sep,
	year = {2014},
	doi = {10.48550/arXiv.1409.4842},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/skywolfmo/Zotero/storage/7JSQMXR4/Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf:application/pdf;arXiv.org Snapshot:/home/skywolfmo/Zotero/storage/H8MDJI2C/1409.html:text/html},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2022-06-29},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}

@misc{he_identity_2016,
	title = {Identity {Mappings} in {Deep} {Residual} {Networks}},
	url = {http://arxiv.org/abs/1603.05027},
	abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
	urldate = {2022-06-29},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jul,
	year = {2016},
	doi = {10.48550/arXiv.1603.05027},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/skywolfmo/Zotero/storage/RQCT2U99/He et al. - 2016 - Identity Mappings in Deep Residual Networks.pdf:application/pdf;arXiv.org Snapshot:/home/skywolfmo/Zotero/storage/9X39RYYX/1603.html:text/html},
}

@misc{xie_aggregated_2017,
	title = {Aggregated {Residual} {Transformations} for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1611.05431},
	abstract = {We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.},
	urldate = {2022-06-29},
	publisher = {arXiv},
	author = {Xie, Saining and Girshick, Ross and Dollár, Piotr and Tu, Zhuowen and He, Kaiming},
	month = apr,
	year = {2017},
	doi = {10.48550/arXiv.1611.05431},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/skywolfmo/Zotero/storage/HXM6CCR7/Xie et al. - 2017 - Aggregated Residual Transformations for Deep Neura.pdf:application/pdf;arXiv.org Snapshot:/home/skywolfmo/Zotero/storage/HGVCZY75/1611.html:text/html},
}

@article{srivastava_dropout_2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	volume = {15},
	url = {http://jmlr.org/papers/v15/srivastava14a.html},
	number = {56},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958},
}

@misc{veit_residual_2016,
	title = {Residual {Networks} {Behave} {Like} {Ensembles} of {Relatively} {Shallow} {Networks}},
	url = {http://arxiv.org/abs/1605.06431},
	abstract = {In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.},
	urldate = {2022-06-29},
	publisher = {arXiv},
	author = {Veit, Andreas and Wilber, Michael and Belongie, Serge},
	month = oct,
	year = {2016},
	doi = {10.48550/arXiv.1605.06431},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/skywolfmo/Zotero/storage/IGIRBXML/Veit et al. - 2016 - Residual Networks Behave Like Ensembles of Relativ.pdf:application/pdf;arXiv.org Snapshot:/home/skywolfmo/Zotero/storage/7T8EGCDV/1605.html:text/html},
}

@misc{feng_overview_2017,
	title = {An {Overview} of {ResNet} and its {Variants}},
	url = {https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035},
	abstract = {After the celebrated victory of AlexNet [1] at the LSVRC2012 classification contest, deep Residual Network [2] was arguably the most…},
	language = {en},
	urldate = {2022-06-29},
	author = {Feng, Vincent},
	month = jul,
	year = {2017},
	note = {Publication Title: Medium},
	file = {Snapshot:/home/skywolfmo/Zotero/storage/4AVBWWLD/an-overview-of-resnet-and-its-variants-5281e2f56035.html:text/html},
}

@misc{noauthor_cnn_nodate,
	title = {{CNN} {Discriminative} {Localization} and {Saliency} - {MIT}},
	url = {http://cnnlocalization.csail.mit.edu/},
	urldate = {2022-06-29},
	file = {CNN Discriminative Localization and Saliency - MIT:/home/skywolfmo/Zotero/storage/Y5HJ9YL4/cnnlocalization.csail.mit.edu.html:text/html},
}

@article{song_visibility_2021,
	title = {Visibility estimation via deep label distribution learning in cloud environment},
	volume = {10},
	issn = {2192-113X},
	url = {https://doi.org/10.1186/s13677-021-00261-7},
	doi = {10.1186/s13677-021-00261-7},
	abstract = {The visibility estimation of the environment has great research and application value in the fields of production. To estimate the visibility, we can utilize the camera to obtain some images as evidence. However, the camera only solves the image acquisition problem, and the analysis of image visibility requires strong computational power. To realize effective and efficient visibility estimation, we employ the cloud computing technique to realize high-through image analysis. Our method combines cloud computing and image-based visibility estimation into a powerful and efficient monitoring framework. To train an accurate model for visibility estimation, it is important to obtain the precise ground truth for every image. However, the ground-truth visibility is difficult to be labeled due to its high ambiguity. To solve this problem, we associate a label distribution to each image. The label distribution contains all the possible visibilities with their probabilities. To learn from such annotation, we employ a CNN-RNN model for visibility-aware feature extraction and a conditional probability neural network for distribution prediction. The estimation result can be improved by fusing the predicting results of multiple images from different views. Our experiment shows that labeling the image with visibility distribution can boost the learning performance, and our method can obtain the visibility from the image efficiently.},
	number = {1},
	urldate = {2022-06-29},
	journal = {Journal of Cloud Computing},
	author = {Song, Mofei and Han, Xu and Liu, Xiao Fan and Li, Qian},
	month = aug,
	year = {2021},
	keywords = {Cloud computing, Deep learning, Label distribution learning, Visibility estimation},
	pages = {46},
	file = {Full Text PDF:/home/skywolfmo/Zotero/storage/JBAYIJVD/Song et al. - 2021 - Visibility estimation via deep label distribution .pdf:application/pdf},
}

@misc{noauthor_causes_nodate,
	title = {Causes of {Visibility} {Impairment}},
	url = {https://webcam.srs.fs.fed.us/impacts/visibility/index.shtml},
	urldate = {2022-06-29},
	file = {Causes of Visibility Impairment:/home/skywolfmo/Zotero/storage/L4W43RSI/index.html:text/html},
}

@article{li_meteorological_2017,
	title = {Meteorological {Visibility} {Evaluation} on {Webcam} {Weather} {Image} {Using} {Deep} {Learning} {Features}},
	doi = {10.7763/IJCTE.2017.V9.1186},
	abstract = {The result shows that the proposed automatic method with deep learning feature is workable for visibility prediction, whose accuracy is higher than that of the traditional method with hand-crafted features. The estimation methods of meteorological visibility currently in use on digital image are mainly based on the meteorological laws and the corresponding features are extracted manually to calculate the visibility. However, besides the parameters of camera setting, the environmental and weather conditions will also affect the image quality and cause different kinds of noise to influence the evaluation accuracy. It is hard to extract all these factors manually and involve them into a certain equation to solve the visibility. Therefore, it is necessary to intelligently extract the useful factors and evaluate visibility. In this paper, an intelligent digital method is proposed to estimate the visibility on webcam weather image. In this method, a pre-trained convolutional neural network (CNN) is employed to automatically extract the visibility features instead of manual method and a generalized regression neural network (GRNN) is designed for intelligent visibility evaluation based on these deep learning features. A series of weather photo with the ground truth of visibility is used to train and test the proposed method. The result shows that the proposed automatic method with deep learning feature is workable for visibility prediction, whose accuracy is higher than that of the traditional method with hand-crafted features.},
	author = {Li, Shengyan and Fu, Hong and Lo, W.},
	year = {2017},
}

@inproceedings{palvanov_dhcnn_2018,
	title = {{DHCNN} for {Visibility} {Estimation} in {Foggy} {Weather} {Conditions}},
	doi = {10.1109/SCIS-ISIS.2018.00050},
	abstract = {This paper proposes a new method to estimate visibility range in strong foggy weather conditions on a basis of the Deep Hybrid Convolutional Neural Network (DHCNN). Our method is designed to estimate visibility distance from a digital camera in real-time but by way of using deep networks, it becomes a more challenging task to achieve outcomes quickly. In addition to this, prior to making any prediction, the model needs to pre-process each input so it will produce the desired results. As a consequence, our implemented prototype consists of two main stage: pre-processing inputs and classifier. Each of those stages concatenated sequentially. From the outer perspective, this demonstrates our model's architecture very deep and computationally costly. However, these two stages make our model more robust and help to learn only useful features from inputs. Since the first pre-processing stage identifies Region of Interest (ROI) and removes redundant parts from a high-resolution image and sends forward to classifier just ROI part in lower resolution. We witnessed great accuracy in estimating visibility on not only heavy foggy images but also the classification of hazy images fulfilled very accurately.},
	booktitle = {2018 {Joint} 10th {International} {Conference} on {Soft} {Computing} and {Intelligent} {Systems} ({SCIS}) and 19th {International} {Symposium} on {Advanced} {Intelligent} {Systems} ({ISIS})},
	author = {Palvanov, Akmaljon and Im Cho, Young},
	month = dec,
	year = {2018},
	keywords = {Cameras, Convolutional neural networks, Image edge detection, Laplace equations, Meteorology, Real-time systems, Streaming media, Visibility, fog, Laplacian of Gaussian filter, deep convolutional neural network, region of interest, edge detection, CCTV cameras},
	pages = {240--243},
	file = {IEEE Xplore Abstract Record:/home/skywolfmo/Zotero/storage/EW9KURMQ/8716200.html:text/html},
}

@article{you_relative_2019,
	title = {Relative {CNN}-{RNN}: {Learning} {Relative} {Atmospheric} {Visibility} {From} {Images}},
	volume = {28},
	issn = {1941-0042},
	shorttitle = {Relative {CNN}-{RNN}},
	doi = {10.1109/TIP.2018.2857219},
	abstract = {We propose a deep learning approach for directly estimating relative atmospheric visibility from outdoor photos without relying on weather images or data that require expensive sensing or custom capture. Our data-driven approach capitalizes on a large collection of Internet images to learn rich scene and visibility varieties. The relative CNN-RNN coarse-to-fine model, where CNN stands for convolutional neural network and RNN stands for recurrent neural network, exploits the joint power of relative support vector machine, which has a good ranking representation, and the data-driven deep learning features derived from our novel CNN-RNN model. The CNN-RNN model makes use of shortcut connections to bridge a CNN module and an RNN coarse-to-fine module. The CNN captures the global view while the RNN simulates human's attention shift, namely, from the whole image (global) to the farthest discerned region (local). The learned relative model can be adapted to predict absolute visibility in limited scenarios. Extensive experiments and comparisons are performed to verify our method. We have built an annotated dataset consisting of about 40000 images with 0.2 million human annotations. The large-scale, annotated visibility data set will be made available to accompany this paper.},
	language = {eng},
	number = {1},
	journal = {IEEE transactions on image processing: a publication of the IEEE Signal Processing Society},
	author = {You, Yang and Lu, Cewu and Wang, Weiming and Tang, Chi-Keung},
	month = jan,
	year = {2019},
	pmid = {30028702},
	pages = {45--55},
}

@article{choi_automatic_2018,
	title = {Automatic {Sea} {Fog} {Detection} and {Estimation} of {Visibility} {Distance} on {CCTV}},
	issn = {0749-0208},
	url = {https://doi.org/10.2112/SI85-177.1},
	doi = {10.2112/SI85-177.1},
	abstract = {Choi, Y.; Choe, H.-G.; Choi, J.Y.; Kim, K.T.; Kim, J.-B., and Kim, N.-I., 2018. Automatic sea fog detection and estimation of visibility distance on CCTV. In: Shim, J.-S.; Chun, I., and Lim, H.S. (eds.), Proceedings from the International Coastal Symposium (ICS) 2018 (Busan, Republic of Korea). Journal of Coastal Research, Special Issue No. 85, pp. 881–885. Coconut Creek (Florida), ISSN 0749-0208.Sea fog is one of the major maritime disasters and thus causes social costs such as transport accidents, mainly due to the reduction of visibility. However, the optical fog sensors are heavily cost so that sea fog detection system is generally difficult to install in practical applications. In this paper, we present a new technique for detecting sea fog and measuring visibility distances using Closed-circuit television (CCTV). Our research is focused on the problem of detecting daytime sea fog and estimating visibility distances in an automatic way. To this end, we exploit that (1) the image analysis based on the HSL (Hue, Saturation, and Lightness) color model is effective for evaluating the density of sea fog and (2) the movement detection system relying on the variance of the pixel values is capable of deciding the visibility distances. The proposed method has advantage of being readily applicable to the widely used CCTV system without additional devices. This paper also explores the possibility of sea fog detection using deep-learning framework. In our method, Deep Convolution Neural Network (DCNN) end-to-end learning solution has been designed and tested for evaluating sea fog detection performance in the course of popular artificial intelligence framework.},
	number = {85 (10085)},
	urldate = {2022-06-29},
	journal = {Journal of Coastal Research},
	author = {Choi, Youngjin and Choe, Hyeong-Gu and Choi, Jae Young and Kim, Kyeong Tae and Kim, Jong-Beom and Kim, Nam-Il},
	month = may,
	year = {2018},
	pages = {881--885},
}

@article{lu_method_2019,
	title = {A method of visibility forecast based on hierarchical sparse representation},
	volume = {58},
	issn = {1047-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S104732031830302X},
	doi = {10.1016/j.jvcir.2018.11.029},
	abstract = {This paper proposes a visibility forecast method based on hierarchical sparse representations. Firstly, it selects meteorological factors from the data of 138 ground stations located in Beijing, Tianjin and Hebei during the months (Oct.-to-Dec. and January) of years 2002–2016. Then, it uses fuzzy C means algorithm (FCM) to construct historical databases containing 5000 samples. Finally, it takes the meteorological factors corresponding to visibility as the sample of historical databases, and uses a hierarchical sparse representation to predict the visibility of new inputs. Experiment, conducted with the data of European Centre for Medium-Range Weather Forecasts (ECMWF), indicates a better performance of the hierarchical sparse representation in contrast to a sparse representation. And the visibility forecast based on hierarchical sparse representation is better than Beijing Regional Environmental Meteorology Prediction System (BREMPS) and BP neural network. The hierarchical sparse representation is simple and easy to expand, which improves the accuracy and reduce the absolute error, which is convenience for other meteorological analysis.},
	language = {en},
	urldate = {2022-06-29},
	journal = {Journal of Visual Communication and Image Representation},
	author = {Lu, Zhenyu and Lu, Bingjian and Zhang, Hengde and Fu, You and Qiu, Yunan and Zhan, Tianming},
	month = jan,
	year = {2019},
	keywords = {Visibility, FCM, Predict, Sparse representation},
	pages = {160--165},
}

@article{outay_estimating_2021,
	title = {Estimating ambient visibility in the presence of fog: a deep convolutional neural network approach},
	volume = {25},
	issn = {1617-4917},
	shorttitle = {Estimating ambient visibility in the presence of fog},
	url = {https://doi.org/10.1007/s00779-019-01334-w},
	doi = {10.1007/s00779-019-01334-w},
	abstract = {Next-generation intelligent transportation systems are based on the acquisition of ambient data that influence traffic flow and safety. Among these, is the ambient visibility range whose estimation, in the presence of fog, is extremely useful for next-generation intelligent transportation systems. However, existing camera-based approaches are based on “engineered features” extraction methods that use computer algorithms and procedures from the image processing field. In this contribution, a novel approach to estimate visibility range under foggy weather conditions is proposed which is based on “learned features” instead. More precisely, we use AlexNet deep convolutional neural network (DCNN), trained with raw image data, for feature extraction and a support vector machine (SVM) for visibility range estimation. Our quantitative analysis showed that the proposed approach is very promising in estimating the visibility range with very good accuracy. The proposed solution can pave the way towards intelligent driveway assistance systems to enhance awareness of driving weather conditions and hence mitigate the safety risks emanating from fog-induced low visibility conditions.},
	language = {en},
	number = {1},
	urldate = {2022-06-29},
	journal = {Personal and Ubiquitous Computing},
	author = {Outay, Fatma and Taha, Bilal and Chaabani, Hazar and Kamoun, Faouzi and Werghi, Naoufel and Yasar, Ansar -Ul-Haque},
	month = feb,
	year = {2021},
	keywords = {Ambient intelligence, Atmospheric visibility, Deep convolutional neural networks, Intelligent transportation systems, Road safety, Ubiquitous technologies},
	pages = {51--62},
}

@article{department_of_computer_science_chu_hai_college_of_higher_education_80_castle_peak_road_castle_peak_bay_tuen_mun_nt_hong_kong_meteorology_2020,
	title = {Meteorology {Visibility} {Estimation} by {Using} {Multi}-{Support} {Vector} {Regression} {Method}},
	issn = {17982340},
	url = {http://www.jait.us/index.php?m=content&c=index&a=show&catid=198&id=1091},
	doi = {10.12720/jait.11.2.40-47},
	urldate = {2022-06-29},
	journal = {Journal of Advances in Information Technology},
	author = {{Department of Computer Science, Chu Hai College of Higher Education, 80 Castle Peak Road, Castle Peak Bay, Tuen Mun, N.T. Hong Kong} and Lo, Wai Lun and Zhu, Meimei and Fu, Hong},
	year = {2020},
	pages = {40--47},
}

@misc{noauthor_instrument_2022,
	title = {Instrument meteorological conditions},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Instrument_meteorological_conditions&oldid=1091609094},
	abstract = {In aviation, instrument meteorological conditions (IMC) is a flight category that describes weather conditions that require pilots to fly primarily by reference to instruments, and therefore under instrument flight rules (IFR), rather than by outside visual references under visual flight rules (VFR). Typically, this means flying in cloudy or bad weather. Pilots sometimes train to fly in these conditions with the aid of products like Foggles, which are specialized glasses that restrict outside vision, forcing the student to rely on instrument indications only.},
	language = {en},
	urldate = {2022-06-29},
	journal = {Wikipedia},
	month = jun,
	year = {2022},
	note = {Page Version ID: 1091609094},
}

@misc{mahmood_what_2021,
	title = {What does {Weights} \& {Biases} do?},
	url = {https://towardsdatascience.com/what-does-weights-biases-do-c060ce6b4b8e},
	abstract = {A platform for enabling a collaborative MLOps culture},
	language = {en},
	urldate = {2022-06-29},
	journal = {Medium},
	author = {Mahmood, Omer},
	month = nov,
	year = {2021},
	file = {Snapshot:/home/skywolfmo/Zotero/storage/XF4PB6XD/what-does-weights-biases-do-c060ce6b4b8e.html:text/html},
}

@misc{noauthor_keras_2022,
	title = {Keras {ImageDataGenerator} {\textbar} {What} is keras {ImageDataGenerator}?},
	url = {https://www.educba.com/keras-imagedatagenerator/},
	abstract = {Guide to Keras ImageDataGenerator. Here we discuss the Introduction, What is Keras ImageDataGenerator, How to use Keras ImageDataGenerator?},
	language = {en-US},
	urldate = {2022-06-29},
	journal = {EDUCBA},
	month = apr,
	year = {2022},
	file = {Snapshot:/home/skywolfmo/Zotero/storage/6J35PVAU/keras-imagedatagenerator.html:text/html},
}

@misc{noauthor_imagedatagenerator_nodate,
	title = {{ImageDataGenerator}},
	url = {https://deepchecks.com/glossary/imagedatagenerator/},
	language = {en},
	urldate = {2022-06-29},
	journal = {Deepchecks},
}

@misc{noauthor_scikit-learn_2022,
	title = {scikit-learn},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Scikit-learn&oldid=1065626908},
	abstract = {Scikit-learn (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language.
It features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.},
	language = {en},
	urldate = {2022-06-29},
	journal = {Wikipedia},
	month = jan,
	year = {2022},
	note = {Page Version ID: 1065626908},
}

@misc{noauthor_matplotlib_2022,
	title = {Matplotlib},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Matplotlib&oldid=1076489986},
	abstract = {Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK. There is also a procedural "pylab" interface based on a state machine (like OpenGL), designed to closely resemble that of MATLAB, though its use is discouraged. SciPy makes use of Matplotlib.
Matplotlib was originally written by John D. Hunter. Since then it has an active development community and is distributed under a BSD-style license. Michael Droettboom was nominated as matplotlib's lead developer shortly before John Hunter's death in August 2012 and was further joined by Thomas Caswell. Matplotlib is a NumFOCUS fiscally sponsored project.Matplotlib 2.0.x supports Python versions 2.7 through 3.10. Python 3 support started with Matplotlib 1.2. Matplotlib 1.4 is the last version to support Python 2.6. Matplotlib has pledged not to support Python 2 past 2020 by signing the Python 3 Statement.},
	language = {en},
	urldate = {2022-06-29},
	journal = {Wikipedia},
	month = mar,
	year = {2022},
	note = {Page Version ID: 1076489986},
}

@misc{noauthor_python_2022,
	title = {Python (programming language)},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Python_(programming_language)&oldid=1095253303},
	abstract = {Python is a high-level, interpreted, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.Python is dynamically-typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a "batteries included" language due to its comprehensive standard library.Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000 and introduced new features such as list comprehensions, cycle-detecting garbage collection, reference counting, and Unicode support. Python 3.0, released in 2008, was a major revision that is not completely backward-compatible with earlier versions. Python 2 was discontinued with version 2.7.18 in 2020.Python consistently ranks as one of the most popular programming languages.},
	language = {en},
	urldate = {2022-06-30},
	journal = {Wikipedia},
	month = jun,
	year = {2022},
	note = {Page Version ID: 1095253303},
}

@misc{noauthor_opencv_2022,
	title = {{OpenCV}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=OpenCV&oldid=1072564905},
	abstract = {OpenCV (Open Source Computer Vision Library) is a library of programming functions mainly aimed at real-time computer vision. Originally developed by Intel, it was later supported by Willow Garage then Itseez (which was later acquired by Intel). The library is cross-platform and free for use under the open-source Apache 2 License. Starting with 2011, OpenCV features GPU acceleration for real-time operations.},
	language = {en},
	urldate = {2022-06-30},
	journal = {Wikipedia},
	month = feb,
	year = {2022},
	note = {Page Version ID: 1072564905},
}

@article{kim_comparison_2018,
	title = {The comparison of visibility measurement between image-based visual range, human eye-based visual range, and meteorological optical range},
	volume = {190},
	issn = {1352-2310},
	url = {https://www.sciencedirect.com/science/article/pii/S1352231018304679},
	doi = {10.1016/j.atmosenv.2018.07.020},
	abstract = {Before the 21st century, visibility monitoring techniques are mainly categorized by human eye-based and optical-based measurements. In order to automate and objectify the former measurement, the latter measurement method has been developed and meteorological optical range (MOR) is used as a concept similar to the visual range. However, in the 21st century, MOR has been found to have some differences with visual range that is recognized by human beings. As image analysis technology is improved, a variety of image-based on-site visibility measurement techniques are being developed using a camera. In this study, image-based visibility measurement that is based on the measured distance to visible objects was performed at about 3 km northwest of the Seoul Meteorological Observatory where the human eye-based and the optical-based visibility monitoring are observed simultaneously. As a result of correlation analysis between three measurement data, the image-based visual range (IVR) was found to be more comparable to the human eye-based visual range (HVR) than to the optical-based MOR. HVR tended to be lower than IVR as visibility improved, but MOR was predominantly higher than IVR as visibility was reduced. Perceived visibility measurement of IVR and HVR were revealed to be more accurate in severe poor visibility conditions. In this study, the system uncertainty of IVR estimation is expected to be within 10\% up to about 33 km and approach 20\% under above a hundred km of visibility. The Visual Range Meter (VRM) program developed for IVR estimation was made up of an individual measurement mode and a real-time measurement mode. The operational errors of approximately 15\% excluding precipitation and other meteorological effects occurred in real-time IVR estimation using the VRM.},
	language = {en},
	urldate = {2022-06-30},
	journal = {Atmospheric Environment},
	author = {Kim, Kyung Won},
	month = oct,
	year = {2018},
	keywords = {Visibility, Camera-based, Image analysis, Meteorological optical range, Visual range},
	pages = {74--86},
	file = {ScienceDirect Snapshot:/home/skywolfmo/Zotero/storage/IC78BEPV/S1352231018304679.html:text/html},
}

@misc{noauthor_fog_2021,
	title = {Fog},
	url = {https://skybrary.aero/articles/fog},
	abstract = {Description Mist and Fog are the terms used to describe low visibility caused by water droplets suspended in the air. Mist is a term used to describe visibility of greater than 1 km while Fog is the term used when visibility is less than 1 km. Fog is effectively surface cloud and has a significant impact on the conduct of flying operations, particularly landing and take-off. There are many different types of fog defined according to how they are formed. Radiation Fog Early morning radiation fog over central Brussels, October 2007.},
	language = {en},
	urldate = {2022-06-30},
	journal = {SKYbrary Aviation Safety},
	month = jul,
	year = {2021},
	file = {Snapshot:/home/skywolfmo/Zotero/storage/S4BVK5XN/fog.html:text/html},
}

@misc{noauthor_smog_nodate,
	title = {smog {\textbar} {National} {Geographic} {Society}},
	url = {https://education.nationalgeographic.org/resource/smog},
	abstract = {Smog is air pollution that reduces visibility},
	urldate = {2022-06-30},
	file = {Snapshot:/home/skywolfmo/Zotero/storage/L8XBRALG/smog.html:text/html},
}

@misc{noauthor_classification_nodate,
	title = {Classification of {Neural} {Network} {\textbar} {Top} 7 {Types} of {Basic} {Neural} {Networks}},
	url = {https://www.educba.com/classification-of-neural-network/},
	urldate = {2022-06-30},
	file = {Classification of Neural Network | Top 7 Types of Basic Neural Networks:/home/skywolfmo/Zotero/storage/V87NHD99/classification-of-neural-network.html:text/html},
}

@article{noauthor_ml_nodate,
	title = {{ML} {Cheatsheet} {Documentation}},
	language = {en},
	pages = {231},
	file = {ML Cheatsheet Documentation.pdf:/home/skywolfmo/Zotero/storage/CNF8DJDY/ML Cheatsheet Documentation.pdf:application/pdf},
}

@misc{mishra_convolutional_2020,
	title = {Convolutional {Neural} {Networks}, {Explained}},
	url = {https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939},
	abstract = {Let’s build your first CNN model},
	language = {en},
	urldate = {2022-06-30},
	journal = {Medium},
	author = {Mishra, Mayank},
	month = sep,
	year = {2020},
	file = {Snapshot:/home/skywolfmo/Zotero/storage/HX6A8MGK/convolutional-neural-networks-explained-9cc5188c4939.html:text/html},
}

@misc{babu_understanding_2021,
	title = {Understanding and {Analyzing} {Deep} {Neural} {Networks}},
	url = {https://medium.com/geekculture/understanding-and-analyzing-deep-neural-networks-a2a7ef737511},
	abstract = {Deep Neural Networks (DNNs) have gained utmost importance these days due to their ability to solve complex engineering problems and…},
	language = {en},
	urldate = {2022-06-30},
	journal = {Geek Culture},
	author = {Babu, Sheema Murugesh},
	month = apr,
	year = {2021},
	file = {Snapshot:/home/skywolfmo/Zotero/storage/4XG5ZZIE/understanding-and-analyzing-deep-neural-networks-a2a7ef737511.html:text/html},
}

@misc{noauthor_ml_nodate-1,
	title = {{ML} {Interview} {Questions} {\textbar} {PDF} {\textbar} {Principal} {Component} {Analysis} {\textbar} {Logistic} {Regression}},
	url = {https://www.scribd.com/document/444350635/ML-interview-questions-docx},
	urldate = {2022-06-30},
}

@misc{noauthor_machine_nodate-1,
	title = {Machine {Learning} {Glossary}  {\textbar}  {Google} {Developers}},
	url = {https://developers.google.com/machine-learning/glossary/},
	urldate = {2022-06-30},
}

@article{kohavi_glossary_1998,
	title = {Glossary of terms. {Special} issue of applications of machine learning and the knowledge discovery process},
	volume = {30},
	journal = {Mach. Learn.},
	author = {Kohavi, Ron and Provost, F.},
	month = jan,
	year = {1998},
	file = {Full Text PDF:/home/skywolfmo/Zotero/storage/EZFPLGLG/Kohavi and Provost - 1998 - Glossary of terms. Special issue of applications o.pdf:application/pdf},
}

@misc{noauthor_relationship_nodate,
	title = {The relationship between {Precision}-{Recall} and {ROC} curves {\textbar} {Proceedings} of the 23rd international conference on {Machine} learning},
	url = {https://dl.acm.org/doi/10.1145/1143844.1143874},
	urldate = {2022-06-30},
	file = {The relationship between Precision-Recall and ROC curves | Proceedings of the 23rd international conference on Machine learning:/home/skywolfmo/Zotero/storage/BT7Z9ZAK/1143844.html:text/html},
}

@misc{noauthor_improving_nodate,
	title = {Improving image quality in poor visibility conditions using a physical model for contrast degradation - {PubMed}},
	url = {https://pubmed.ncbi.nlm.nih.gov/18267391/},
	urldate = {2022-06-30},
	file = {Improving image quality in poor visibility conditions using a physical model for contrast degradation - PubMed:/home/skywolfmo/Zotero/storage/92WR4BGX/18267391.html:text/html},
}

@misc{noauthor_physics-based_nodate,
	title = {Physics-based approach to color image enhancement in poor visibility conditions},
	url = {https://opg.optica.org/josaa/abstract.cfm?uri=josaa-18-10-2460},
	urldate = {2022-06-30},
	file = {Physics-based approach to color image enhancement in poor visibility conditions:/home/skywolfmo/Zotero/storage/ZVE7JLSU/abstract.html:text/html},
}

@inproceedings{sen_object_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Networks} and {Systems}},
	title = {Object {Detection} in {Foggy} {Weather} {Conditions}},
	isbn = {978-3-030-93247-3},
	doi = {10.1007/978-3-030-93247-3_70},
	abstract = {In this paper, we addressed the problem of object detection under foggy weather in an outdoor environment. State-of-the-art object detection schemes perform very well in normal weather conditions but many of them fail when it comes to adverse weather. We proposed a novel approach to remove fog followed by object detection using PP-YOLO. Removal of fog is achieved using a U-shaped network with residual feedback from one layer to another. The network resembles an encoder-decoder scheme which enables us to reduce noise in the form of fog. Defogged images are then fed to the object detection network i.e. PP-YOLO. For the training purpose, we synthesized a dataset of foggy images and also used other existing datasets. Experimental results show the efficiency of the proposed scheme when compared to many existing methods for fog removal. Ablation studies show that the inclusion of the fog removal method certainly improves the detection performance.},
	language = {en},
	booktitle = {Intelligent {Computing} \& {Optimization}},
	publisher = {Springer International Publishing},
	author = {Sen, Prithwish and Das, Anindita and Sahu, Nilkanta},
	editor = {Vasant, Pandian and Zelinka, Ivan and Weber, Gerhard-Wilhelm},
	year = {2022},
	keywords = {Encoder-decoder, Fog removal, Loss, Object detection, PP-YOLO, Residual block, U-shaped network},
	pages = {728--737},
}

@inproceedings{he_single_2009,
	title = {Single image haze removal using dark channel prior},
	doi = {10.1109/CVPR.2009.5206515},
	abstract = {In this paper, we propose a simple but effective image prior - dark channel prior to remove haze from a single input image. The dark channel prior is a kind of statistics of the haze-free outdoor images. It is based on a key observation - most local patches in haze-free outdoor images contain some pixels which have very low intensities in at least one color channel. Using this prior with the haze imaging model, we can directly estimate the thickness of the haze and recover a high quality haze-free image. Results on a variety of outdoor haze images demonstrate the power of the proposed prior. Moreover, a high quality depth map can also be obtained as a by-product of haze removal.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {He, Kaiming and Sun, Jian and Tang, Xiaoou},
	month = jun,
	year = {2009},
	note = {ISSN: 1063-6919},
	keywords = {Pixel, Statistics},
	pages = {1956--1963},
	file = {IEEE Xplore Abstract Record:/home/skywolfmo/Zotero/storage/SQ878BNH/5206515.html:text/html},
}

@article{chaabani_estimating_2018,
	series = {The 9th {International} {Conference} on {Emerging} {Ubiquitous} {Systems} and {Pervasive} {Networks} ({EUSPN}-2018) / {The} 8th {International} {Conference} on {Current} and {Future} {Trends} of {Information} and {Communication} {Technologies} in {Healthcare} ({ICTH}-2018) / {Affiliated} {Workshops}},
	title = {Estimating meteorological visibility range under foggy weather conditions: {A} deep learning approach},
	volume = {141},
	issn = {1877-0509},
	shorttitle = {Estimating meteorological visibility range under foggy weather conditions},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050918317885},
	doi = {10.1016/j.procs.2018.10.139},
	abstract = {Systems capable of estimating visibility distances under foggy weather conditions are extremely useful for next-generation cooperative situational awareness and collision avoidance systems. In this paper, we present a brief review of noticeable approaches for determining visibility distance under foggy weather conditions. We then propose a novel approach based on the combination of a deep learning method for feature extraction and an SVM classifier. We present a quantitative evaluation of the proposed solution and show that our approach provides better performance results compared to an earlier approach that was based on the combination of an ANN model and a set of global feature descriptors. Our experimental results show that the proposed solution presents very promising results in support for next-generation situational awareness and cooperative collision avoidance systems. Hence it can potentially contribute towards safer driving conditions in the presence of fog.},
	language = {en},
	urldate = {2022-06-30},
	journal = {Procedia Computer Science},
	author = {Chaabani, Hazar and Werghi, Naoufel and Kamoun, Faouzi and Taha, Bilal and Outay, Fatma and Yasar, Ansar-Ul-Haque},
	month = jan,
	year = {2018},
	keywords = {computer vision, convolution neural networks, deep learning, intelligent transportation systems, machine learning, meteorologcal visibility, neural networks, Visibility distance},
	pages = {478--483},
	file = {Full Text:/home/skywolfmo/Zotero/storage/WQ4QEQGP/Chaabani et al. - 2018 - Estimating meteorological visibility range under f.pdf:application/pdf},
}

@article{li_method_2019,
	title = {A {Method} of {Visibility} {Detection} {Based} on the {Transfer} {Learning}},
	volume = {36},
	doi = {10.1175/JTECH-D-19-0025.1},
	abstract = {Atmospheric visibility is an important element of meteorological observation. With existing methods, defining image features that reflect visibility accurately and comprehensively is difficult. This paper proposes a visibility detection method based on transfer learning using deep convolutional neural networks (DCNN) that addresses issues caused by a lack of sufficient visibility labeled datasets. In the proposed method, each image was first divided into several subregions, which were encoded to extract visual features using a pretrained no-reference image quality assessment neural network. Then a support vector regression model was trained to map the extracted features to the visibility. The fusion weight of each subregion was evaluated according to the error analysis of the regression model. Finally, the neural network was fine-tuned to better fit the problem of visibility detection using the current detection results conversely. Experimental results demonstrated that the detection accuracy of the proposed method exceeds 90\% and satisfies the requirements of daily observation applications.},
	journal = {Journal of Atmospheric and Oceanic Technology},
	author = {Li, Qian and Tang, Shaoen and Peng, Xuan and Ma, Qiang},
	month = aug,
	year = {2019},
	file = {Full Text PDF:/home/skywolfmo/Zotero/storage/SDTHWGTW/Li et al. - 2019 - A Method of Visibility Detection Based on the Tran.pdf:application/pdf},
}

@misc{noauthor_faa_2008,
	title = {{FAA} - {Instrument} {Flying} {Handbook}},
	url = {https://web.archive.org/web/20080422015034/http://www.faa.gov/library/manuals/aviation/instrument_flying_handbook/},
	urldate = {2022-06-30},
	month = apr,
	year = {2008},
	file = {Snapshot:/home/skywolfmo/Zotero/storage/G3WX9J6B/instrument_flying_handbook.html:text/html},
}

@misc{srivastava_training_2015,
	title = {Training {Very} {Deep} {Networks}},
	url = {http://arxiv.org/abs/1507.06228},
	doi = {10.48550/arXiv.1507.06228},
	abstract = {Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.},
	urldate = {2022-06-30},
	publisher = {arXiv},
	author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, Jürgen},
	month = nov,
	year = {2015},
	note = {arXiv:1507.06228 [cs]},
	keywords = {Computer Science - Machine Learning, 68T01, Computer Science - Neural and Evolutionary Computing, G.1.6, I.2.6},
	file = {arXiv Fulltext PDF:/home/skywolfmo/Zotero/storage/3I683BZF/Srivastava et al. - 2015 - Training Very Deep Networks.pdf:application/pdf;arXiv.org Snapshot:/home/skywolfmo/Zotero/storage/VT98G3IT/1507.html:text/html},
}
@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@Article{app11030997,
AUTHOR = {Li, Jiaping and Lo, Wai Lun and Fu, Hong and Chung, Henry Shu Hung},
TITLE = {A Transfer Learning Method for Meteorological Visibility Estimation Based on Feature Fusion Method},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {997},
URL = {https://www.mdpi.com/2076-3417/11/3/997},
ISSN = {2076-3417},
DOI = {10.3390/app11030997}
}





@misc{yang2024depth,
      title={Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data}, 
      author={Lihe Yang and Bingyi Kang and Zilong Huang and Xiaogang Xu and Jiashi Feng and Hengshuang Zhao},
      year={2024},
      eprint={2401.10891},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@ARTICLE{6817512,
  author={Chen, Xue-Wen and Lin, Xiaotong},
  journal={IEEE Access}, 
  title={Big Data Deep Learning: Challenges and Perspectives}, 
  year={2014},
  volume={2},
  number={},
  pages={514-525},
  keywords={Machine learning;Pattern recognition;Big data;Natural language processing;Data processing;Information analysis;Classifier design and evaluation;feature representation;machine learning;neural nets models;parallel processing},
  doi={10.1109/ACCESS.2014.2325029}}

@misc{bachmann2022multimae,
      title={MultiMAE: Multi-modal Multi-task Masked Autoencoders}, 
      author={Roman Bachmann and David Mizrahi and Andrei Atanov and Amir Zamir},
      year={2022},
      eprint={2204.01678},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{wang2024largescale,
      title={Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey}, 
      author={Xiao Wang and Guangyao Chen and Guangwu Qian and Pengcheng Gao and Xiao-Yong Wei and Yaowei Wang and Yonghong Tian and Wen Gao},
      year={2024},
      eprint={2302.10035},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{ranftl2021vision,
      title={Vision Transformers for Dense Prediction}, 
      author={René Ranftl and Alexey Bochkovskiy and Vladlen Koltun},
      year={2021},
      eprint={2103.13413},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@misc{akkus2023multimodal,
      title={Multimodal Deep Learning}, 
      author={Cem Akkus and Luyang Chu and Vladana Djakovic and Steffen Jauch-Walser and Philipp Koch and Giacomo Loss and Christopher Marquardt and Marco Moldovan and Nadja Sauter and Maximilian Schneider and Rickmer Schulte and Karol Urbanczyk and Jann Goschenhofer and Christian Heumann and Rasmus Hvingelby and Daniel Schalk and Matthias Aßenmacher},
      year={2023},
      eprint={2301.04856},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{radford2021learning,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{jia2021scaling,
      title={Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision}, 
      author={Chao Jia and Yinfei Yang and Ye Xia and Yi-Ting Chen and Zarana Parekh and Hieu Pham and Quoc V. Le and Yunhsuan Sung and Zhen Li and Tom Duerig},
      year={2021},
      eprint={2102.05918},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@article{huang_fusion_2020,
	title = {Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines},
	volume = {3},
	issn = {2398-6352},
	url = {https://doi.org/10.1038/s41746-020-00341-z},
	doi = {10.1038/s41746-020-00341-z},
	number = {1},
	journal = {npj Digital Medicine},
	author = {Huang, Shih-Cheng and Pareek, Anuj and Seyyedi, Saeed and Banerjee, Imon and Lungren, Matthew P.},
	month = oct,
	year = {2020},
	pages = {136},
}

@book{visibilitybook,
author = {Malm, W.C.},
year = {2016},
month = {01},
pages = {1-333},
title = {Visibility: The Seeing of Near and Distant Landscape Features}
}

@article{gui2023comprehensive,
  title={A comprehensive survey and taxonomy on single image dehazing based on deep learning},
  author={Gui, Jie and Cong, Xiaofeng and Cao, Yuan and Ren, Wenqi and Zhang, Jun and Zhang, Jing and Cao, Jiuxin and Tao, Dacheng},
  journal={ACM Computing Surveys},
  volume={55},
  number={13s},
  pages={1--37},
  year={2023},
  publisher={ACM New York, NY}
}

@misc{zadeh2017tensorfusionnetworkmultimodal,
      title={Tensor Fusion Network for Multimodal Sentiment Analysis}, 
      author={Amir Zadeh and Minghai Chen and Soujanya Poria and Erik Cambria and Louis-Philippe Morency},
      year={2017},
      eprint={1707.07250},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1707.07250}, 
}

@misc{liu2018efficientlowrankmultimodalfusion,
      title={Efficient Low-rank Multimodal Fusion with Modality-Specific Factors}, 
      author={Zhun Liu and Ying Shen and Varun Bharadhwaj Lakshminarasimhan and Paul Pu Liang and Amir Zadeh and Louis-Philippe Morency},
      year={2018},
      eprint={1806.00064},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1806.00064}, 
}

@inproceedings{NEURIPS2021_76ba9f56,
 author = {Nagrani, Arsha and Yang, Shan and Arnab, Anurag and Jansen, Aren and Schmid, Cordelia and Sun, Chen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {14200--14213},
 publisher = {Curran Associates, Inc.},
 title = {Attention Bottlenecks for Multimodal Fusion},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/76ba9f564ebbc35b1014ac498fafadd0-Paper.pdf},
 volume = {34},
 year = {2021}
}


@article{fultz2016fatal,
  title={Fatal weather-related general aviation accidents in the United States},
  author={Fultz, Andrew J and Ashley, Walker S},
  journal={Physical Geography},
  volume={37},
  number={5},
  pages={291--312},
  year={2016},
  publisher={Taylor \& Francis}
}



@inproceedings{Kulesa2003WEATHERAA,
  title={WEATHER AND AVIATION: HOW DOES WEATHER AFFECT THE SAFETY AND OPERATIONS OF AIRPORTS AND AVIATION, AND HOW DOES FAA WORK TO MANAGE WEATHER-RELATED EFFECTS?},
  author={Gloria Kulesa},
  year={2003},
  url={https://api.semanticscholar.org/CorpusID:108023423}
}




@article{long2022analysis,
  title={Analysis of weather-related accident and incident data associated with Section 14 CFR Part 91 Operations},
  author={Long, Thomas},
  journal={The Collegiate Aviation Review International},
  volume={40},
  number={1},
  year={2022}
}


@article{fujita1977analysis,
  title={An analysis of three weather-related aircraft accidents},
  author={Fujita, T Theodore and Caracena, Fernando},
  journal={Bulletin of the American Meteorological Society},
  volume={58},
  number={11},
  pages={1164--1181},
  year={1977},
  publisher={American Meteorological Society}
}


@inproceedings{ramee2021analysis,
  title={Analysis of weather-related helicopter accidents and incidents in the United States},
  author={Ramee, Coline and Speirs, Andrew and Payan, Alexia P and Mavris, Dimitri},
  booktitle={AIAA Aviation 2021 Forum},
  pages={2954},
  year={2021}
}


@misc{huang2021makesmultimodallearningbetter,
      title={What Makes Multi-modal Learning Better than Single (Provably)}, 
      author={Yu Huang and Chenzhuang Du and Zihui Xue and Xuanyao Chen and Hang Zhao and Longbo Huang},
      year={2021},
      eprint={2106.04538},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.04538}, 
}

@misc{liang2024foundationsmultisensoryartificialintelligence,
      title={Foundations of Multisensory Artificial Intelligence}, 
      author={Paul Pu Liang},
      year={2024},
      eprint={2404.18976},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.18976}, 
}

@techreport{ahlstrom2019assessments,
  title        = {Assessments of Flight and Weather Conditions during General Aviation Operations},
  author       = {Ahlstrom, Ulf and Racine, Nicole and Hallman, Kevin},
  year         = {2019},
  institution  = {Federal Aviation Administration, William J. Hughes Technical Center},
  number       = {DOT/FAA/TC-19/33},
  address      = {Atlantic City International Airport, NJ},
  note         = {Available from the Federal Aviation Administration William J. Hughes Technical Center: \url{https://actlibrary.tc.faa.gov}}
}
